{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Numpy 1.16 has memory leak bug  https://github.com/numpy/numpy/issues/13808\n",
      "It is recommended to downgrade to numpy 1.15 or older\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, GATConv, GINConv, global_max_pool, GlobalAttention, GatedGraphConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.utils.convert import from_scipy_sparse_matrix\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "\n",
    "from pyscf import gto, scf, tools, ao2mo\n",
    "\n",
    "\n",
    "import train\n",
    "from graph_model import SecondNet, SimpleNet, THCNet\n",
    "from preprocess import build_qm7, build_thc_graph\n",
    "from train import train, test\n",
    "from thc import THCContainer\n",
    "from utils import khatri_rao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis = 'sto-3g'\n",
    "# basis = 'cc-pvdz'\n",
    "mols = build_qm7(basis)\n",
    "mols = mols[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/prime/lib/python3.7/site-packages/pyscf/scf/chkfile.py:31: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  with h5py.File(chkfile) as fh5:\n",
      "/anaconda3/envs/prime/lib/python3.7/site-packages/pyscf/lib/misc.py:874: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  h5py.File.__init__(self, filename, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rho L2: 8.739769471082871e-17\n",
      "T_ao L_infinity: 3.5419481285062613\n",
      "T_mo L_infinity: 3.5011169696917284\n",
      "T_mo L_2: 4.6800868603748444\n",
      "rho L2: 6.407716821774357e-17\n",
      "T_ao L_infinity: 3.5419423271946395\n",
      "T_mo L_infinity: 1.9263477573051362\n",
      "T_mo L_2: 7.577257103125808\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'grid_points_per_atom': 10, 'epsilon_qr': 1e-15, 'epsilon_inv': 1e-15, 'verbose': True}\n",
    "mol_data = [THCContainer(mol, kwargs) for mol in mols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_J loss 0.0064803640378401535\n",
      "E loss 0.004153715623926188\n",
      "MP2_J loss 0.01275178495774533\n",
      "MP2 loss 0.00714553245258371\n",
      "(5, 4)\n",
      "\n",
      "E_J loss 0.013447644992301163\n",
      "E loss 0.00890297329760615\n",
      "MP2_J loss 0.014744573909468234\n",
      "MP2 loss 0.012345363714010585\n",
      "(9, 7)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "for con in mol_data:\n",
    "    \n",
    "    print(\"E_J loss\", np.linalg.norm(con.E[0] - con.E_THC[0]))\n",
    "    print(\"E loss\", np.linalg.norm(con.E[2] - con.E_THC[2]))\n",
    "    print(\"MP2_J loss\", np.linalg.norm(con.MP2[0] - con.MP2_THC[0]))\n",
    "    print(\"MP2 loss\", np.linalg.norm(con.MP2[2] - con.MP2_THC[2]))\n",
    "    print(con.E[2].shape)\n",
    "    print(\"\")\n",
    "    \n",
    "    data = build_thc_graph(con)\n",
    "#     data = Data(X = torch.from_numpy(X), Z = torch.from_numpy(Z),\n",
    "#                 U = torch.from_numpy(U), coords = torch.from_numpy(coords),\n",
    "#                 T_ao = torch.from_numpy(T_ao), T_mo = torch.from_numpy(T_mo),\n",
    "#                mol = mol)\n",
    "    \n",
    "    dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.9923,  2.0000,  1.0000, 12.2999,  1.0000,  0.2306,  0.0000],\n",
      "       dtype=torch.float64)\n",
      "tensor([-1.9682e+01, -1.3914e-15,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -1.7698e-01,  0.0000e+00], dtype=torch.float64)\n",
      "tensor([-2.1529e-02,  4.1536e-03,  3.9252e-03,  3.9469e+00,  7.3000e-03,\n",
      "         3.8903e-04,  0.0000e+00], dtype=torch.float64)\n",
      "\n",
      "tensor([ 1.2833,  2.0000,  1.0000, 16.5238,  1.0000,  0.2691,  0.0000],\n",
      "       dtype=torch.float64)\n",
      "tensor([-2.2032e+01, -1.7099e-15,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -2.6083e-01,  0.0000e+00], dtype=torch.float64)\n",
      "tensor([-1.4205e-02,  1.7728e-03,  1.6414e-03,  4.7341e+00,  3.7050e-03,\n",
      "         7.3472e-04,  0.0000e+00], dtype=torch.float64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data in dataset:\n",
    "    edge_attr = data.edge_attr\n",
    "    print(torch.max(edge_attr, dim = 0)[0])\n",
    "    print(torch.min(edge_attr, dim = 0)[0])\n",
    "    print(torch.mean(edge_attr, dim = 0))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(model, loader, lr = 0.003, iterations = 10, verbose = False, lamb = 1.0, device = torch.device(\"cpu\")):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    losses = []\n",
    "    for i in range(iterations):\n",
    "        batch_losses = []\n",
    "        for data in loader:\n",
    "                        \n",
    "            E_THC = data.con.E_THC[0] # first term means the J term\n",
    "            E_THC = torch.from_numpy(E_THC)\n",
    "            E_hat = model(data)[data.E_mask][:,0].reshape(E_THC.shape)\n",
    "            E_pred = E_THC + lamb * E_hat\n",
    "            \n",
    "            E_true = data.con.E[0] # first term means the J term\n",
    "            E_true = torch.from_numpy(E_true)\n",
    "            \n",
    "            loss = torch.norm(E_true - E_pred) / torch.norm(E_true) #Scale regularization\n",
    "                    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            optimizer.step()\n",
    "                        \n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "        batch_loss = np.mean(np.array(batch_losses))\n",
    "        losses.append(batch_loss)\n",
    "        if verbose:\n",
    "            print(\"timestep: {}, loss: {:e}\".format(i, batch_loss))\n",
    "    \n",
    "    model.eval()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 0, loss: 6.035934e-01\n",
      "timestep: 1, loss: 5.435130e-01\n",
      "timestep: 2, loss: 4.867439e-01\n",
      "timestep: 3, loss: 4.318143e-01\n",
      "timestep: 4, loss: 3.785291e-01\n",
      "timestep: 5, loss: 3.261483e-01\n",
      "timestep: 6, loss: 2.737901e-01\n",
      "timestep: 7, loss: 2.205453e-01\n",
      "timestep: 8, loss: 1.670143e-01\n",
      "timestep: 9, loss: 1.160370e-01\n",
      "timestep: 10, loss: 8.060720e-02\n",
      "timestep: 11, loss: 8.322713e-02\n",
      "timestep: 12, loss: 1.010259e-01\n",
      "timestep: 13, loss: 1.048495e-01\n",
      "timestep: 14, loss: 9.327526e-02\n",
      "timestep: 15, loss: 7.500714e-02\n",
      "timestep: 16, loss: 6.306815e-02\n",
      "timestep: 17, loss: 6.444906e-02\n",
      "timestep: 18, loss: 6.998090e-02\n",
      "timestep: 19, loss: 7.155316e-02\n",
      "timestep: 20, loss: 6.858520e-02\n",
      "timestep: 21, loss: 6.423701e-02\n",
      "timestep: 22, loss: 6.235106e-02\n",
      "timestep: 23, loss: 6.359390e-02\n",
      "timestep: 24, loss: 6.479669e-02\n",
      "timestep: 25, loss: 6.377614e-02\n",
      "timestep: 26, loss: 6.155884e-02\n",
      "timestep: 27, loss: 6.032448e-02\n",
      "timestep: 28, loss: 6.066803e-02\n",
      "timestep: 29, loss: 6.138520e-02\n",
      "timestep: 30, loss: 6.143923e-02\n",
      "timestep: 31, loss: 6.092944e-02\n",
      "timestep: 32, loss: 6.052986e-02\n",
      "timestep: 33, loss: 6.054429e-02\n",
      "timestep: 34, loss: 6.064640e-02\n",
      "timestep: 35, loss: 6.047536e-02\n",
      "timestep: 36, loss: 6.012872e-02\n",
      "timestep: 37, loss: 5.994011e-02\n",
      "timestep: 38, loss: 5.999345e-02\n",
      "timestep: 39, loss: 6.009014e-02\n",
      "timestep: 40, loss: 6.006496e-02\n",
      "timestep: 41, loss: 5.996541e-02\n",
      "timestep: 42, loss: 5.990133e-02\n",
      "timestep: 43, loss: 5.987892e-02\n",
      "timestep: 44, loss: 5.983708e-02\n",
      "timestep: 45, loss: 5.976071e-02\n",
      "timestep: 46, loss: 5.970059e-02\n",
      "timestep: 47, loss: 5.968696e-02\n",
      "timestep: 48, loss: 5.969177e-02\n",
      "timestep: 49, loss: 5.967440e-02\n",
      "timestep: 50, loss: 5.963777e-02\n",
      "timestep: 51, loss: 5.961188e-02\n",
      "timestep: 52, loss: 5.958716e-02\n",
      "timestep: 53, loss: 5.955383e-02\n",
      "timestep: 54, loss: 5.951480e-02\n",
      "timestep: 55, loss: 5.948650e-02\n",
      "timestep: 56, loss: 5.946602e-02\n",
      "timestep: 57, loss: 5.945273e-02\n",
      "timestep: 58, loss: 5.943033e-02\n",
      "timestep: 59, loss: 5.940297e-02\n",
      "timestep: 60, loss: 5.937219e-02\n",
      "timestep: 61, loss: 5.934150e-02\n",
      "timestep: 62, loss: 5.931533e-02\n",
      "timestep: 63, loss: 5.928971e-02\n",
      "timestep: 64, loss: 5.926402e-02\n",
      "timestep: 65, loss: 5.924181e-02\n",
      "timestep: 66, loss: 5.921886e-02\n",
      "timestep: 67, loss: 5.919404e-02\n",
      "timestep: 68, loss: 5.917120e-02\n",
      "timestep: 69, loss: 5.914746e-02\n",
      "timestep: 70, loss: 5.911939e-02\n",
      "timestep: 71, loss: 5.909584e-02\n",
      "timestep: 72, loss: 5.907542e-02\n",
      "timestep: 73, loss: 5.905297e-02\n",
      "timestep: 74, loss: 5.903165e-02\n",
      "timestep: 75, loss: 5.900779e-02\n",
      "timestep: 76, loss: 5.898383e-02\n",
      "timestep: 77, loss: 5.896215e-02\n",
      "timestep: 78, loss: 5.893593e-02\n",
      "timestep: 79, loss: 5.891136e-02\n",
      "timestep: 80, loss: 5.888778e-02\n",
      "timestep: 81, loss: 5.886219e-02\n",
      "timestep: 82, loss: 5.883319e-02\n",
      "timestep: 83, loss: 5.880407e-02\n",
      "timestep: 84, loss: 5.878268e-02\n",
      "timestep: 85, loss: 5.875759e-02\n",
      "timestep: 86, loss: 5.872811e-02\n",
      "timestep: 87, loss: 5.869570e-02\n",
      "timestep: 88, loss: 5.866831e-02\n",
      "timestep: 89, loss: 5.863817e-02\n",
      "timestep: 90, loss: 5.861030e-02\n",
      "timestep: 91, loss: 5.857920e-02\n",
      "timestep: 92, loss: 5.854368e-02\n",
      "timestep: 93, loss: 5.850658e-02\n",
      "timestep: 94, loss: 5.847505e-02\n",
      "timestep: 95, loss: 5.843875e-02\n",
      "timestep: 96, loss: 5.841098e-02\n",
      "timestep: 97, loss: 5.837309e-02\n",
      "timestep: 98, loss: 5.833271e-02\n",
      "timestep: 99, loss: 5.829893e-02\n",
      "timestep: 100, loss: 5.828021e-02\n",
      "timestep: 101, loss: 5.824229e-02\n",
      "timestep: 102, loss: 5.819686e-02\n",
      "timestep: 103, loss: 5.817618e-02\n",
      "timestep: 104, loss: 5.815656e-02\n",
      "timestep: 105, loss: 5.812842e-02\n",
      "timestep: 106, loss: 5.810003e-02\n",
      "timestep: 107, loss: 5.807442e-02\n",
      "timestep: 108, loss: 5.804254e-02\n",
      "timestep: 109, loss: 5.802541e-02\n",
      "timestep: 110, loss: 5.799268e-02\n",
      "timestep: 111, loss: 5.797297e-02\n",
      "timestep: 112, loss: 5.794950e-02\n",
      "timestep: 113, loss: 5.791719e-02\n",
      "timestep: 114, loss: 5.787281e-02\n",
      "timestep: 115, loss: 5.785587e-02\n",
      "timestep: 116, loss: 5.783809e-02\n",
      "timestep: 117, loss: 5.780231e-02\n",
      "timestep: 118, loss: 5.776675e-02\n",
      "timestep: 119, loss: 5.773912e-02\n",
      "timestep: 120, loss: 5.771047e-02\n",
      "timestep: 121, loss: 5.767488e-02\n",
      "timestep: 122, loss: 5.764502e-02\n",
      "timestep: 123, loss: 5.761227e-02\n",
      "timestep: 124, loss: 5.758963e-02\n",
      "timestep: 125, loss: 5.756190e-02\n",
      "timestep: 126, loss: 5.752424e-02\n",
      "timestep: 127, loss: 5.750185e-02\n",
      "timestep: 128, loss: 5.747779e-02\n",
      "timestep: 129, loss: 5.744457e-02\n",
      "timestep: 130, loss: 5.742043e-02\n",
      "timestep: 131, loss: 5.738497e-02\n",
      "timestep: 132, loss: 5.734434e-02\n",
      "timestep: 133, loss: 5.732377e-02\n",
      "timestep: 134, loss: 5.730338e-02\n",
      "timestep: 135, loss: 5.726544e-02\n",
      "timestep: 136, loss: 5.723944e-02\n",
      "timestep: 137, loss: 5.720439e-02\n",
      "timestep: 138, loss: 5.715579e-02\n",
      "timestep: 139, loss: 5.715319e-02\n",
      "timestep: 140, loss: 5.712929e-02\n",
      "timestep: 141, loss: 5.709104e-02\n",
      "timestep: 142, loss: 5.705035e-02\n",
      "timestep: 143, loss: 5.702088e-02\n",
      "timestep: 144, loss: 5.698698e-02\n",
      "timestep: 145, loss: 5.694731e-02\n",
      "timestep: 146, loss: 5.692739e-02\n",
      "timestep: 147, loss: 5.689250e-02\n",
      "timestep: 148, loss: 5.686377e-02\n",
      "timestep: 149, loss: 5.682820e-02\n",
      "timestep: 150, loss: 5.679373e-02\n",
      "timestep: 151, loss: 5.675956e-02\n",
      "timestep: 152, loss: 5.673632e-02\n",
      "timestep: 153, loss: 5.669421e-02\n",
      "timestep: 154, loss: 5.663976e-02\n",
      "timestep: 155, loss: 5.663614e-02\n",
      "timestep: 156, loss: 5.661298e-02\n",
      "timestep: 157, loss: 5.656095e-02\n",
      "timestep: 158, loss: 5.654063e-02\n",
      "timestep: 159, loss: 5.650482e-02\n",
      "timestep: 160, loss: 5.643291e-02\n",
      "timestep: 161, loss: 5.637630e-02\n",
      "timestep: 162, loss: 5.637067e-02\n",
      "timestep: 163, loss: 5.632699e-02\n",
      "timestep: 164, loss: 5.629683e-02\n",
      "timestep: 165, loss: 5.625171e-02\n",
      "timestep: 166, loss: 5.618452e-02\n",
      "timestep: 167, loss: 5.618200e-02\n",
      "timestep: 168, loss: 5.615561e-02\n",
      "timestep: 169, loss: 5.610321e-02\n",
      "timestep: 170, loss: 5.603931e-02\n",
      "timestep: 171, loss: 5.598976e-02\n",
      "timestep: 172, loss: 5.592997e-02\n",
      "timestep: 173, loss: 5.591687e-02\n",
      "timestep: 174, loss: 5.585556e-02\n",
      "timestep: 175, loss: 5.582108e-02\n",
      "timestep: 176, loss: 5.576236e-02\n",
      "timestep: 177, loss: 5.570850e-02\n",
      "timestep: 178, loss: 5.570210e-02\n",
      "timestep: 179, loss: 5.566573e-02\n",
      "timestep: 180, loss: 5.558337e-02\n",
      "timestep: 181, loss: 5.552688e-02\n",
      "timestep: 182, loss: 5.547620e-02\n",
      "timestep: 183, loss: 5.544134e-02\n",
      "timestep: 184, loss: 5.537920e-02\n",
      "timestep: 185, loss: 5.530443e-02\n",
      "timestep: 186, loss: 5.528402e-02\n",
      "timestep: 187, loss: 5.525355e-02\n",
      "timestep: 188, loss: 5.515634e-02\n",
      "timestep: 189, loss: 5.509038e-02\n",
      "timestep: 190, loss: 5.500023e-02\n",
      "timestep: 191, loss: 5.496913e-02\n",
      "timestep: 192, loss: 5.496515e-02\n",
      "timestep: 193, loss: 5.490029e-02\n",
      "timestep: 194, loss: 5.480407e-02\n",
      "timestep: 195, loss: 5.474594e-02\n",
      "timestep: 196, loss: 5.467071e-02\n",
      "timestep: 197, loss: 5.462432e-02\n",
      "timestep: 198, loss: 5.457817e-02\n",
      "timestep: 199, loss: 5.450899e-02\n"
     ]
    }
   ],
   "source": [
    "vertex_dim = dataset[0].x.shape[1]\n",
    "edge_dim = dataset[0].edge_attr.shape[1]\n",
    "hidden_dim = 20\n",
    "model = THCNet(vertex_dim, edge_dim, hidden_dim).double()\n",
    "\n",
    "lr = 0.001\n",
    "verbose = True\n",
    "lamb = 1e-1\n",
    "\n",
    "losses = train(model, dataset, iterations = 200, lr = lr, verbose = verbose, lamb = lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:prime] *",
   "language": "python",
   "name": "conda-env-prime-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
