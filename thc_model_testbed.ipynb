{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Numpy 1.16 has memory leak bug  https://github.com/numpy/numpy/issues/13808\n",
      "It is recommended to downgrade to numpy 1.15 or older\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, GATConv, GINConv, global_max_pool, GlobalAttention, GatedGraphConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.utils.convert import from_scipy_sparse_matrix\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "\n",
    "from pyscf import gto, scf, tools, ao2mo\n",
    "\n",
    "\n",
    "import train\n",
    "from graph_model import SecondNet, SimpleNet, THCNet\n",
    "from preprocess import build_qm7, build_thc_graph\n",
    "from train import train, test\n",
    "from hf import get_data, save_data, load_data, thc_to_eri_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = build_qm7('sto-3g')\n",
    "mols = mols[0:2]\n",
    "filename = \"sto33\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/prime/lib/python3.7/site-packages/pyscf/scf/chkfile.py:31: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  with h5py.File(chkfile) as fh5:\n",
      "/anaconda3/envs/prime/lib/python3.7/site-packages/pyscf/lib/misc.py:874: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  h5py.File.__init__(self, filename, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rho L2: 1.653473217268125e-14\n",
      "Sanity C as khatri-rao product: 0.0\n",
      "T_ao L_infinity: 0.00017613996788602444\n",
      "T_mo L_infinity: 0.00016804775254997129\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'grid_points_per_atom': 300, 'epsilon_qr': 1e-15, 'epsilon_inv': 1e-15, 'verbose': True}\n",
    "save_data(mols, filename, force = True, kwargs = kwargs)\n",
    "mol_data = load_data(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_khatri_rao(X, Y):\n",
    "    C = torch.einsum(\"ij,kj->ikj\",X,Y)\n",
    "    C = C.view(X.shape[0] * Y.shape[0], -1)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00016804775254997129\n",
      "0.0008740056166136256\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "for m in mol_data:\n",
    "    \n",
    "    X, Z, U, T_ao, T_mo, coords, mol = m.X, m.Z, m.U, m.T_ao, m.T_mo, m.coords, gto.mole.loads(m.mol)\n",
    "                \n",
    "    C = np.einsum(\"ij,kj->ikj\",U.T @ X,U.T @ X).reshape((X.shape[0] ** 2, -1))\n",
    "    T_approx = C @ Z @ C.T\n",
    "    T_double = T_mo.reshape((X.shape[0] ** 2, X.shape[0] ** 2))\n",
    "    print(np.max(np.abs(T_double - T_approx)))\n",
    "    print(np.linalg.norm(T_double - T_approx))\n",
    "    \n",
    "#     data = build_thc_graph(m)\n",
    "    data = Data(X = torch.from_numpy(X), Z = torch.from_numpy(Z),\n",
    "                U = torch.from_numpy(U), coords = torch.from_numpy(coords),\n",
    "                T_ao = torch.from_numpy(T_ao), T_mo = torch.from_numpy(T_mo),\n",
    "               mol = mol)\n",
    "    \n",
    "    dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# class PointNet(nn.Module):\n",
    "#     def __init__(self, X, with_Y = False):        \n",
    "#         super(PointNet, self).__init__()\n",
    "        \n",
    "#         self.X = nn.Parameter(torch.Tensor(X.shape[0], X.shape[1]))\n",
    "#         nn.init.kaiming_uniform_(self.X, a=math.sqrt(5))\n",
    "        \n",
    "#         if with_Y:\n",
    "#             self.Y = nn.Parameter(torch.Tensor(X.shape[0], X.shape[1]))\n",
    "#             nn.init.kaiming_uniform_(self.Y, a=math.sqrt(5))\n",
    "#         else:\n",
    "#             self.Y = self.X\n",
    "                        \n",
    "#     def forward(self, mol):\n",
    "#         return self.X, self.Y\n",
    "    \n",
    "# class GTO(torch.autograd.Function):\n",
    "\n",
    "#     @staticmethod\n",
    "#     def forward(ctx, input, mol):\n",
    "\n",
    "#         ctx.save_for_backward(input)\n",
    "#         ctx.mol = mol\n",
    "#         ao_value = mol.eval_gto(\"GTOval_sph\", input.cpu().numpy())\n",
    "#         return torch.from_numpy(ao_value).T\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx, grad_output):\n",
    "\n",
    "#         input, = ctx.saved_tensors\n",
    "#         mol = ctx.mol\n",
    "        \n",
    "#         ao_value = mol.eval_gto(\"GTOval_sph\", input.cpu().numpy())\n",
    "        \n",
    "#         ao_grad = mol.eval_gto(\"GTOval_ip_sph\", input.cpu().numpy())\n",
    "#         ao_grad = torch.from_numpy(ao_grad)\n",
    "#         ao_grad = torch.transpose(ao_grad, 1, 2)\n",
    "        \n",
    "        \n",
    "#         grad_input = torch.diagonal(grad_output.T.unsqueeze(0) @ ao_grad, dim1 = 1, dim2 = 2)\n",
    "\n",
    "#         return grad_input.T, None\n",
    "    \n",
    "# phi = GTO.apply\n",
    "\n",
    "# class SpatialPointNet(nn.Module):\n",
    "#     def __init__(self, coords):        \n",
    "#         super(SpatialPointNet, self).__init__()\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             self.coords = nn.Parameter(coords.clone())\n",
    "                        \n",
    "#     def forward(self, mol):\n",
    "#         X = phi(self.coords, mol)\n",
    "#         return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(model, loader, lr = 0.003, iterations = 10, criterion = nn.MSELoss(),\n",
    "          verbose = False, device = torch.device(\"cpu\"), lamb = None, epsilon = 1e-15, grad_clip = None):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    losses = []\n",
    "    for i in range(iterations):\n",
    "        for data in loader:\n",
    "            \n",
    "            M, n = data.X.shape\n",
    "            \n",
    "            T_ao = data.T_ao.view(M ** 2, M ** 2)\n",
    "            T_mo = data.T_mo.view(M ** 2, M ** 2)\n",
    "\n",
    "            X_hat, Y_hat = model(data.mol)\n",
    "            if lamb is not None:\n",
    "                X_hat = lamb * X_hat + data.X\n",
    "                Y_hat = lamb * Y_hat + data.X\n",
    "            \n",
    "            T_approx_fixed = thc_to_eri_torch(data.X, data.U, T_ao, epsilon)\n",
    "            T_approx_learned = thc_to_eri_torch(X_hat, data.U, T_ao, epsilon, Y_hat)\n",
    "\n",
    "            \n",
    "            loss = torch.norm(T_approx_learned - T_mo)\n",
    "                    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "                        \n",
    "            if grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            \n",
    "            optimizer.step()\n",
    "                        \n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if verbose:\n",
    "                print(\"timestep: {}, loss: {:e}\".format(i, loss.item()))\n",
    "                \n",
    "            if i == iterations - 1:\n",
    "                print(\"FINAL\")\n",
    "                print(\"loss: {:e}\".format(loss.item()))\n",
    "                fixed_loss = torch.norm(T_approx_fixed - T_mo).item()\n",
    "                print(\"fixed: {:e}\".format(fixed_loss))\n",
    "    \n",
    "    model.eval()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PointNet(dataset[0].X).double()\n",
    "lr = 0.008\n",
    "lamb = 1e-4\n",
    "\n",
    "# model = SpatialPointNet(dataset[0].coords).double()\n",
    "# lr = 0.001\n",
    "# lamb = None\n",
    "\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 0, loss: 5.024740e-03\n",
      "timestep: 1, loss: 3.051993e-03\n",
      "timestep: 2, loss: 1.101305e-03\n",
      "timestep: 3, loss: 8.779428e-04\n",
      "timestep: 4, loss: 7.249134e-04\n",
      "timestep: 5, loss: 6.337369e-04\n",
      "timestep: 6, loss: 5.093174e-04\n",
      "timestep: 7, loss: 5.412347e-04\n",
      "timestep: 8, loss: 3.089312e-04\n",
      "timestep: 9, loss: 4.264008e-04\n",
      "timestep: 10, loss: 3.656393e-04\n",
      "timestep: 11, loss: 3.719416e-04\n",
      "timestep: 12, loss: 3.323896e-04\n",
      "timestep: 13, loss: 2.428713e-04\n",
      "timestep: 14, loss: 2.499967e-04\n",
      "timestep: 15, loss: 2.754783e-04\n",
      "timestep: 16, loss: 1.990220e-04\n",
      "timestep: 17, loss: 1.484193e-04\n",
      "timestep: 18, loss: 2.612016e-04\n",
      "timestep: 19, loss: 2.170164e-04\n",
      "timestep: 20, loss: 1.399709e-04\n",
      "timestep: 21, loss: 2.221091e-04\n",
      "timestep: 22, loss: 2.157565e-04\n",
      "timestep: 23, loss: 1.578819e-04\n",
      "timestep: 24, loss: 2.337199e-04\n",
      "timestep: 25, loss: 1.458551e-04\n",
      "timestep: 26, loss: 2.072459e-04\n",
      "timestep: 27, loss: 1.595805e-04\n",
      "timestep: 28, loss: 1.365809e-04\n",
      "timestep: 29, loss: 1.678807e-04\n",
      "timestep: 30, loss: 1.316489e-04\n",
      "timestep: 31, loss: 1.368732e-04\n",
      "timestep: 32, loss: 1.437974e-04\n",
      "timestep: 33, loss: 1.038487e-04\n",
      "timestep: 34, loss: 9.625395e-05\n",
      "timestep: 35, loss: 1.259047e-04\n",
      "timestep: 36, loss: 1.089107e-04\n",
      "timestep: 37, loss: 8.533714e-05\n",
      "timestep: 38, loss: 1.197400e-04\n",
      "timestep: 39, loss: 9.099772e-05\n",
      "timestep: 40, loss: 9.242909e-05\n",
      "timestep: 41, loss: 1.237354e-04\n",
      "timestep: 42, loss: 7.718790e-05\n",
      "timestep: 43, loss: 1.037121e-04\n",
      "timestep: 44, loss: 1.162905e-04\n",
      "timestep: 45, loss: 7.405450e-05\n",
      "timestep: 46, loss: 1.162437e-04\n",
      "timestep: 47, loss: 8.455878e-05\n",
      "timestep: 48, loss: 8.448798e-05\n",
      "timestep: 49, loss: 1.184094e-04\n",
      "timestep: 50, loss: 6.791018e-05\n",
      "timestep: 51, loss: 9.980707e-05\n",
      "timestep: 52, loss: 1.025628e-04\n",
      "timestep: 53, loss: 6.414126e-05\n",
      "timestep: 54, loss: 1.164461e-04\n",
      "timestep: 55, loss: 8.115903e-05\n",
      "timestep: 56, loss: 7.316326e-05\n",
      "timestep: 57, loss: 1.182063e-04\n",
      "timestep: 58, loss: 6.607382e-05\n",
      "timestep: 59, loss: 9.802078e-05\n",
      "timestep: 60, loss: 9.376158e-05\n",
      "timestep: 61, loss: 6.333263e-05\n",
      "timestep: 62, loss: 1.086821e-04\n",
      "timestep: 63, loss: 7.966267e-05\n",
      "timestep: 64, loss: 6.977414e-05\n",
      "timestep: 65, loss: 1.179334e-04\n",
      "timestep: 66, loss: 7.339282e-05\n",
      "timestep: 67, loss: 1.269015e-04\n",
      "timestep: 68, loss: 8.253087e-05\n",
      "timestep: 69, loss: 1.231861e-04\n",
      "timestep: 70, loss: 6.115767e-05\n",
      "timestep: 71, loss: 1.215390e-04\n",
      "timestep: 72, loss: 6.153923e-05\n",
      "timestep: 73, loss: 9.962891e-05\n",
      "timestep: 74, loss: 7.630614e-05\n",
      "timestep: 75, loss: 5.967845e-05\n",
      "timestep: 76, loss: 1.156933e-04\n",
      "timestep: 77, loss: 7.117617e-05\n",
      "timestep: 78, loss: 1.182467e-04\n",
      "timestep: 79, loss: 6.856505e-05\n",
      "timestep: 80, loss: 1.360199e-04\n",
      "timestep: 81, loss: 7.642714e-05\n",
      "timestep: 82, loss: 1.694800e-04\n",
      "timestep: 83, loss: 1.598465e-04\n",
      "timestep: 84, loss: 7.741169e-05\n",
      "timestep: 85, loss: 1.162578e-04\n",
      "timestep: 86, loss: 6.594973e-05\n",
      "timestep: 87, loss: 7.496820e-05\n",
      "timestep: 88, loss: 9.257097e-05\n",
      "timestep: 89, loss: 7.937907e-05\n",
      "timestep: 90, loss: 9.663508e-05\n",
      "timestep: 91, loss: 6.447532e-05\n",
      "timestep: 92, loss: 1.089516e-04\n",
      "timestep: 93, loss: 7.292350e-05\n",
      "timestep: 94, loss: 1.316486e-04\n",
      "timestep: 95, loss: 1.212388e-04\n",
      "timestep: 96, loss: 7.846858e-05\n",
      "timestep: 97, loss: 1.061904e-04\n",
      "timestep: 98, loss: 5.924987e-05\n",
      "timestep: 99, loss: 6.983059e-05\n",
      "timestep: 100, loss: 6.962010e-05\n",
      "timestep: 101, loss: 5.270058e-05\n",
      "timestep: 102, loss: 8.872800e-05\n",
      "timestep: 103, loss: 6.118941e-05\n",
      "timestep: 104, loss: 1.119091e-04\n",
      "timestep: 105, loss: 7.510434e-05\n",
      "timestep: 106, loss: 1.025639e-04\n",
      "timestep: 107, loss: 9.386602e-05\n",
      "timestep: 108, loss: 7.675977e-05\n",
      "timestep: 109, loss: 7.329563e-05\n",
      "timestep: 110, loss: 9.565079e-05\n",
      "timestep: 111, loss: 8.663064e-05\n",
      "timestep: 112, loss: 7.723743e-05\n",
      "timestep: 113, loss: 7.971670e-05\n",
      "timestep: 114, loss: 8.173407e-05\n",
      "timestep: 115, loss: 7.366948e-05\n",
      "timestep: 116, loss: 7.647877e-05\n",
      "timestep: 117, loss: 5.806140e-05\n",
      "timestep: 118, loss: 8.764616e-05\n",
      "timestep: 119, loss: 6.371879e-05\n",
      "timestep: 120, loss: 1.019949e-04\n",
      "timestep: 121, loss: 8.857633e-05\n",
      "timestep: 122, loss: 7.605498e-05\n",
      "timestep: 123, loss: 7.819098e-05\n",
      "timestep: 124, loss: 7.018773e-05\n",
      "timestep: 125, loss: 6.542643e-05\n",
      "timestep: 126, loss: 8.206590e-05\n",
      "timestep: 127, loss: 6.559955e-05\n",
      "timestep: 128, loss: 9.283134e-05\n",
      "timestep: 129, loss: 8.459875e-05\n",
      "timestep: 130, loss: 7.087584e-05\n",
      "timestep: 131, loss: 7.690629e-05\n",
      "timestep: 132, loss: 6.582186e-05\n",
      "timestep: 133, loss: 6.411700e-05\n",
      "timestep: 134, loss: 8.355088e-05\n",
      "timestep: 135, loss: 7.211676e-05\n",
      "timestep: 136, loss: 6.907876e-05\n",
      "timestep: 137, loss: 5.546262e-05\n",
      "timestep: 138, loss: 7.800034e-05\n",
      "timestep: 139, loss: 5.599355e-05\n",
      "timestep: 140, loss: 9.546091e-05\n",
      "timestep: 141, loss: 8.300474e-05\n",
      "timestep: 142, loss: 7.213972e-05\n",
      "timestep: 143, loss: 8.046769e-05\n",
      "timestep: 144, loss: 6.316842e-05\n",
      "timestep: 145, loss: 7.378893e-05\n",
      "timestep: 146, loss: 7.041273e-05\n",
      "timestep: 147, loss: 6.860260e-05\n",
      "timestep: 148, loss: 7.748802e-05\n",
      "timestep: 149, loss: 5.726388e-05\n",
      "timestep: 150, loss: 6.835193e-05\n",
      "timestep: 151, loss: 5.471911e-05\n",
      "timestep: 152, loss: 8.668214e-05\n",
      "timestep: 153, loss: 6.683360e-05\n",
      "timestep: 154, loss: 7.008550e-05\n",
      "timestep: 155, loss: 5.881467e-05\n",
      "timestep: 156, loss: 7.822165e-05\n",
      "timestep: 157, loss: 6.612833e-05\n",
      "timestep: 158, loss: 8.009568e-05\n",
      "timestep: 159, loss: 6.484453e-05\n",
      "timestep: 160, loss: 7.428107e-05\n",
      "timestep: 161, loss: 6.558820e-05\n",
      "timestep: 162, loss: 8.469455e-05\n",
      "timestep: 163, loss: 7.670395e-05\n",
      "timestep: 164, loss: 6.652455e-05\n",
      "timestep: 165, loss: 6.803939e-05\n",
      "timestep: 166, loss: 7.816282e-05\n",
      "timestep: 167, loss: 7.078494e-05\n",
      "timestep: 168, loss: 6.243351e-05\n",
      "timestep: 169, loss: 6.771673e-05\n",
      "timestep: 170, loss: 5.751294e-05\n",
      "timestep: 171, loss: 4.968857e-05\n",
      "timestep: 172, loss: 7.026874e-05\n",
      "timestep: 173, loss: 6.391260e-05\n",
      "timestep: 174, loss: 8.677733e-05\n",
      "timestep: 175, loss: 7.501302e-05\n",
      "timestep: 176, loss: 6.377727e-05\n",
      "timestep: 177, loss: 5.929651e-05\n",
      "timestep: 178, loss: 7.448985e-05\n",
      "timestep: 179, loss: 8.846676e-05\n",
      "timestep: 180, loss: 8.143317e-05\n",
      "timestep: 181, loss: 9.557511e-05\n",
      "timestep: 182, loss: 9.669178e-05\n",
      "timestep: 183, loss: 7.157272e-05\n",
      "timestep: 184, loss: 6.808662e-05\n",
      "timestep: 185, loss: 8.766808e-05\n",
      "timestep: 186, loss: 6.991381e-05\n",
      "timestep: 187, loss: 5.182197e-05\n",
      "timestep: 188, loss: 7.164778e-05\n",
      "timestep: 189, loss: 9.261318e-05\n",
      "timestep: 190, loss: 5.368191e-05\n",
      "timestep: 191, loss: 4.738460e-05\n",
      "timestep: 192, loss: 6.379054e-05\n",
      "timestep: 193, loss: 8.190500e-05\n",
      "timestep: 194, loss: 5.809295e-05\n",
      "timestep: 195, loss: 5.392973e-05\n",
      "timestep: 196, loss: 6.215376e-05\n",
      "timestep: 197, loss: 6.804659e-05\n",
      "timestep: 198, loss: 5.889576e-05\n",
      "timestep: 199, loss: 5.183100e-05\n",
      "FINAL\n",
      "loss: 5.183100e-05\n",
      "fixed: 9.139143e-04\n"
     ]
    }
   ],
   "source": [
    "losses = train(model, dataset, iterations = 200, lr = lr, verbose = verbose, lamb = lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_dim = dataset[0].x.shape[1]\n",
    "edge_dim = dataset[0].edge_attr.shape[1]\n",
    "hidden_dim = 20\n",
    "model = THCNet(vertex_dim, edge_dim, hidden_dim).double()\n",
    "\n",
    "lr = 0.001\n",
    "lamb = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train(model, dataset, iterations = 200, lr = lr, verbose = verbose, lamb = lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:prime] *",
   "language": "python",
   "name": "conda-env-prime-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
