{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Numpy 1.16 has memory leak bug  https://github.com/numpy/numpy/issues/13808\n",
      "It is recommended to downgrade to numpy 1.15 or older\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, GATConv, GINConv, global_max_pool, GlobalAttention, GatedGraphConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.utils.convert import from_scipy_sparse_matrix\n",
    "\n",
    "from pyscf import gto, scf, tools, ao2mo\n",
    "\n",
    "\n",
    "import model\n",
    "import train\n",
    "from model import SecondNet, SimpleNet\n",
    "from preprocess import build_graph, build_qm7\n",
    "from train import train, test\n",
    "from hf import get_data, save_data, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = build_qm7('sto-3g')\n",
    "#Omit first molecule, outlier geometry\n",
    "mols = mols[:20]\n",
    "filename = \"sto33\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/prime/lib/python3.7/site-packages/pyscf/scf/chkfile.py:31: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  with h5py.File(chkfile) as fh5:\n",
      "/anaconda3/envs/prime/lib/python3.7/site-packages/pyscf/lib/misc.py:874: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  h5py.File.__init__(self, filename, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#TODO: Encode number of electrons explicitly\n",
    "#TODO: Encode HF features?\n",
    "#TODO: Encode the \"flavor\" of the orbital basis as features as well\n",
    "\n",
    "save_data(mols, filename, force = True)\n",
    "mol_data = load_data(filename, 'MO')[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#M: Number of orbitals\n",
    "#N: Number of electrons\n",
    "#F: feature vector length\n",
    "\n",
    "#A is potential matrix: M x M\n",
    "#U is coulumb 4-tensor: M x M x M x M\n",
    "#X is additional orbital feature matrix: M x F_1\n",
    "#Y is additional pairwise orbital feature matrix: M x M x F_2\n",
    "\n",
    "#E is ground state energy\n",
    "dataset = []\n",
    "for mol in mol_data:\n",
    "    \n",
    "    A, U, X, Y, P, E, mo_occ = mol\n",
    "                                \n",
    "    data = build_graph(A, U, X, Y, P, E, mo_occ, epsilon = 0.0)\n",
    "\n",
    "    dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "split = int(0.8 * len(dataset))\n",
    "train_loader = DataLoader(dataset[:split], batch_size = 2)\n",
    "test_loader = DataLoader(dataset[split:], batch_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(model)\n",
    "from model import SecondNet, SimpleNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_dim = dataset[0].x.shape[1]\n",
    "edge_dim = dataset[0].edge_attr.shape[1]\n",
    "hidden_dim = 20\n",
    "\n",
    "train_criterion = nn.MSELoss()\n",
    "test_criterion = nn.L1Loss()\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=8, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0015226476972868294\n",
      "-0.004276809273677748\n",
      "-0.00801162633942964\n",
      "timestep: 0, loss: [0.00262387 0.08838134]\n",
      "timestep: 1, loss: [0.00032672 0.01379266]\n",
      "timestep: 2, loss: [0.00042787 0.00546428]\n",
      "timestep: 3, loss: [0.00035114 0.00009623]\n",
      "timestep: 4, loss: [0.00007051 0.00019259]\n",
      "timestep: 5, loss: [0.00006271 0.00022614]\n",
      "timestep: 6, loss: [0.00013323 0.00057993]\n",
      "timestep: 7, loss: [0.00008343 0.00135173]\n",
      "timestep: 8, loss: [0.00001998 0.00099012]\n",
      "timestep: 9, loss: [0.00002506 0.00027853]\n",
      "timestep: 10, loss: [0.00003928 0.00044852]\n",
      "timestep: 11, loss: [0.00002013 0.0001886 ]\n",
      "timestep: 12, loss: [0.00001097 0.00042129]\n",
      "timestep: 13, loss: [0.00001702 0.00083208]\n",
      "timestep: 14, loss: [0.00001365 0.00046179]\n",
      "timestep: 15, loss: [0.0000073  0.00019594]\n",
      "timestep: 16, loss: [0.00000761 0.00014839]\n",
      "timestep: 17, loss: [0.00000934 0.00015797]\n",
      "timestep: 18, loss: [0.00000691 0.00016241]\n",
      "timestep: 19, loss: [0.0000058  0.00017064]\n",
      "timestep: 20, loss: [0.00000647 0.00023794]\n",
      "timestep: 21, loss: [0.00000595 0.00027697]\n",
      "timestep: 22, loss: [0.00000509 0.00019865]\n",
      "timestep: 23, loss: [0.00000532 0.00014339]\n",
      "timestep: 24, loss: [0.00000538 0.00014071]\n",
      "timestep: 25, loss: [0.00000509 0.00017884]\n",
      "timestep: 26, loss: [0.00000501 0.00019186]\n",
      "timestep: 27, loss: [0.00000498 0.0001783 ]\n",
      "timestep: 28, loss: [0.0000048  0.00017285]\n",
      "timestep: 29, loss: [0.00000475 0.00016538]\n",
      "timestep: 30, loss: [0.00000474 0.00015042]\n",
      "timestep: 31, loss: [0.00000467 0.00014739]\n",
      "timestep: 32, loss: [0.00000463 0.00016085]\n",
      "timestep: 33, loss: [0.00000458 0.00016576]\n",
      "timestep: 34, loss: [0.00000449 0.00015352]\n",
      "timestep: 35, loss: [0.00000444 0.00014467]\n",
      "timestep: 36, loss: [0.00000442 0.00014403]\n",
      "timestep: 37, loss: [0.00000437 0.00014329]\n",
      "timestep: 38, loss: [0.00000432 0.00014159]\n",
      "timestep: 39, loss: [0.00000428 0.00014269]\n",
      "timestep: 40, loss: [0.00000424 0.00014121]\n",
      "timestep: 41, loss: [0.00000419 0.00013535]\n",
      "timestep: 42, loss: [0.00000416 0.00013207]\n",
      "timestep: 43, loss: [0.00000413 0.00013291]\n",
      "timestep: 44, loss: [0.00000409 0.00013188]\n",
      "timestep: 45, loss: [0.00000405 0.00012928]\n",
      "timestep: 46, loss: [0.00000402 0.00012815]\n",
      "timestep: 47, loss: [0.00000399 0.00012632]\n",
      "timestep: 48, loss: [0.00000396 0.00012375]\n",
      "timestep: 49, loss: [0.00000393 0.00012261]\n",
      "timestep: 50, loss: [0.0000039  0.00012148]\n",
      "timestep: 51, loss: [0.00000386 0.00011916]\n",
      "timestep: 52, loss: [0.00000384 0.00011761]\n",
      "timestep: 53, loss: [0.00000381 0.00011642]\n",
      "timestep: 54, loss: [0.00000378 0.00011459]\n",
      "timestep: 55, loss: [0.00000375 0.00011311]\n",
      "timestep: 56, loss: [0.00000372 0.00011135]\n",
      "timestep: 57, loss: [0.00000369 0.00010969]\n",
      "timestep: 58, loss: [0.00000367 0.00010843]\n",
      "timestep: 59, loss: [0.00000364 0.00010724]\n",
      "timestep: 60, loss: [0.00000362 0.00010574]\n",
      "timestep: 61, loss: [0.00000359 0.00010412]\n",
      "timestep: 62, loss: [0.00000357 0.00010278]\n",
      "timestep: 63, loss: [0.00000354 0.00010175]\n",
      "timestep: 64, loss: [0.00000352 0.00010025]\n",
      "timestep: 65, loss: [0.0000035  0.00009891]\n",
      "timestep: 66, loss: [0.00000347 0.00009746]\n",
      "timestep: 67, loss: [0.00000345 0.0000959 ]\n",
      "timestep: 68, loss: [0.00000343 0.00009486]\n",
      "timestep: 69, loss: [0.00000341 0.00009369]\n",
      "timestep: 70, loss: [0.00000339 0.00009248]\n",
      "timestep: 71, loss: [0.00000337 0.00009106]\n",
      "timestep: 72, loss: [0.00000335 0.00008992]\n",
      "timestep: 73, loss: [0.00000333 0.00008884]\n",
      "timestep: 74, loss: [0.00000331 0.000088  ]\n",
      "timestep: 75, loss: [0.00000329 0.00008678]\n",
      "timestep: 76, loss: [0.00000327 0.00008565]\n",
      "timestep: 77, loss: [0.00000325 0.00008432]\n",
      "timestep: 78, loss: [0.00000323 0.00008319]\n",
      "timestep: 79, loss: [0.00000322 0.00008233]\n",
      "timestep: 80, loss: [0.0000032  0.00008131]\n",
      "timestep: 81, loss: [0.00000318 0.00008044]\n",
      "timestep: 82, loss: [0.00000317 0.00007932]\n",
      "timestep: 83, loss: [0.00000315 0.00007835]\n",
      "timestep: 84, loss: [0.00000314 0.00007774]\n",
      "timestep: 85, loss: [0.00000311 0.00007582]\n",
      "timestep: 86, loss: [0.0000031  0.00007477]\n",
      "timestep: 87, loss: [0.00000308 0.00007303]\n",
      "timestep: 88, loss: [0.00000306 0.00007223]\n",
      "timestep: 89, loss: [0.00000305 0.00007172]\n",
      "timestep: 90, loss: [0.00000303 0.00006966]\n",
      "timestep: 91, loss: [0.00000302 0.00006945]\n",
      "timestep: 92, loss: [0.00000301 0.0000699 ]\n",
      "timestep: 93, loss: [0.000003   0.00006806]\n",
      "timestep: 94, loss: [0.00000298 0.0000668 ]\n",
      "timestep: 95, loss: [0.00000297 0.00006712]\n",
      "timestep: 96, loss: [0.00000296 0.00006583]\n",
      "timestep: 97, loss: [0.00000294 0.00006528]\n",
      "timestep: 98, loss: [0.00000294 0.00006467]\n",
      "timestep: 99, loss: [0.00000292 0.00006261]\n",
      "timestep: 100, loss: [0.00000291 0.00006317]\n",
      "timestep: 101, loss: [0.0000029  0.00006323]\n",
      "timestep: 102, loss: [0.00000288 0.00006037]\n",
      "timestep: 103, loss: [0.00000287 0.00006027]\n",
      "timestep: 104, loss: [0.00000287 0.00006109]\n",
      "timestep: 105, loss: [0.00000286 0.00005939]\n",
      "timestep: 106, loss: [0.00000284 0.0000585 ]\n",
      "timestep: 107, loss: [0.00000283 0.00005844]\n",
      "timestep: 108, loss: [0.00000282 0.00005725]\n",
      "timestep: 109, loss: [0.00000281 0.00005719]\n",
      "timestep: 110, loss: [0.00000281 0.00005656]\n",
      "timestep: 111, loss: [0.00000279 0.00005488]\n",
      "timestep: 112, loss: [0.00000278 0.00005544]\n",
      "timestep: 113, loss: [0.00000277 0.00005431]\n",
      "timestep: 114, loss: [0.00000276 0.00005373]\n",
      "timestep: 115, loss: [0.00000276 0.00005367]\n",
      "timestep: 116, loss: [0.00000274 0.00005193]\n",
      "timestep: 117, loss: [0.00000273 0.00005257]\n",
      "timestep: 118, loss: [0.00000272 0.00005153]\n",
      "timestep: 119, loss: [0.00000271 0.0000507 ]\n",
      "timestep: 120, loss: [0.00000271 0.00005089]\n",
      "timestep: 121, loss: [0.00000269 0.00004923]\n",
      "timestep: 122, loss: [0.00000268 0.00004967]\n",
      "timestep: 123, loss: [0.00000268 0.00004869]\n",
      "timestep: 124, loss: [0.00000267 0.00004815]\n",
      "timestep: 125, loss: [0.00000267 0.00004816]\n",
      "timestep: 126, loss: [0.00000265 0.00004663]\n",
      "timestep: 127, loss: [0.00000264 0.00004725]\n",
      "timestep: 128, loss: [0.00000264 0.00004608]\n",
      "timestep: 129, loss: [0.00000262 0.00004567]\n",
      "timestep: 130, loss: [0.00000263 0.00004576]\n",
      "timestep: 131, loss: [0.0000026  0.00004407]\n",
      "timestep: 132, loss: [0.0000026 0.0000448]\n",
      "timestep: 133, loss: [0.0000026  0.00004432]\n",
      "timestep: 134, loss: [0.00000258 0.00004247]\n",
      "timestep: 135, loss: [0.00000258 0.0000437 ]\n",
      "timestep: 136, loss: [0.00000257 0.00004236]\n",
      "timestep: 137, loss: [0.00000255 0.00004142]\n",
      "timestep: 138, loss: [0.00000257 0.00004255]\n",
      "timestep: 139, loss: [0.00000255 0.00004068]\n",
      "timestep: 140, loss: [0.00000254 0.00004068]\n",
      "timestep: 141, loss: [0.00000254 0.00004052]\n",
      "timestep: 142, loss: [0.00000253 0.00004001]\n",
      "timestep: 143, loss: [0.00000252 0.00003904]\n",
      "timestep: 144, loss: [0.00000251 0.00003922]\n",
      "timestep: 145, loss: [0.00000251 0.00003856]\n",
      "timestep: 146, loss: [0.00000249 0.0000375 ]\n",
      "timestep: 147, loss: [0.0000025  0.00003861]\n",
      "timestep: 148, loss: [0.00000249 0.00003696]\n",
      "timestep: 149, loss: [0.00000248 0.00003688]\n",
      "timestep: 150, loss: [0.00000248 0.00003683]\n",
      "timestep: 151, loss: [0.00000247 0.00003627]\n",
      "timestep: 152, loss: [0.00000246 0.00003539]\n",
      "timestep: 153, loss: [0.00000245 0.00003564]\n",
      "timestep: 154, loss: [0.00000245 0.00003488]\n",
      "timestep: 155, loss: [0.00000244 0.00003479]\n",
      "timestep: 156, loss: [0.00000244 0.00003412]\n",
      "timestep: 157, loss: [0.00000243 0.00003385]\n",
      "timestep: 158, loss: [0.00000243 0.00003367]\n",
      "timestep: 159, loss: [0.00000241 0.00003232]\n",
      "timestep: 160, loss: [0.00000243 0.00003364]\n",
      "timestep: 161, loss: [0.00000241 0.00003189]\n",
      "timestep: 162, loss: [0.0000024  0.00003167]\n",
      "timestep: 163, loss: [0.00000241 0.00003248]\n",
      "timestep: 164, loss: [0.00000239 0.00003069]\n",
      "timestep: 165, loss: [0.00000239 0.00003085]\n",
      "timestep: 166, loss: [0.00000239 0.00003103]\n",
      "timestep: 167, loss: [0.00000238 0.00003011]\n",
      "timestep: 168, loss: [0.00000237 0.00002961]\n",
      "timestep: 169, loss: [0.00000237 0.00003011]\n",
      "timestep: 170, loss: [0.00000236 0.00002893]\n",
      "timestep: 171, loss: [0.00000235 0.00002855]\n",
      "timestep: 172, loss: [0.00000236 0.00002923]\n",
      "timestep: 173, loss: [0.00000234 0.00002791]\n",
      "timestep: 174, loss: [0.00000235 0.0000284 ]\n",
      "timestep: 175, loss: [0.00000234 0.00002752]\n",
      "timestep: 176, loss: [0.00000233 0.00002733]\n",
      "timestep: 177, loss: [0.00000233 0.00002728]\n",
      "timestep: 178, loss: [0.00000232 0.00002651]\n",
      "timestep: 179, loss: [0.00000232 0.00002624]\n",
      "timestep: 180, loss: [0.00000232 0.00002652]\n",
      "timestep: 181, loss: [0.00000231 0.00002529]\n",
      "timestep: 182, loss: [0.00000231 0.00002576]\n",
      "timestep: 183, loss: [0.00000231 0.00002537]\n",
      "timestep: 184, loss: [0.00000229 0.00002436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 185, loss: [0.0000023 0.0000248]\n",
      "timestep: 186, loss: [0.00000229 0.00002428]\n",
      "timestep: 187, loss: [0.00000229 0.00002413]\n",
      "timestep: 188, loss: [0.00000228 0.0000237 ]\n",
      "timestep: 189, loss: [0.00000228 0.00002347]\n",
      "timestep: 190, loss: [0.00000227 0.00002295]\n",
      "timestep: 191, loss: [0.00000228 0.00002323]\n",
      "timestep: 192, loss: [0.00000227 0.0000224 ]\n",
      "timestep: 193, loss: [0.00000227 0.00002269]\n",
      "timestep: 194, loss: [0.00000226 0.00002183]\n",
      "timestep: 195, loss: [0.00000225 0.0000217 ]\n",
      "timestep: 196, loss: [0.00000226 0.00002185]\n",
      "timestep: 197, loss: [0.00000225 0.00002109]\n",
      "timestep: 198, loss: [0.00000225 0.00002132]\n",
      "timestep: 199, loss: [0.00000224 0.00002036]\n",
      "timestep: 200, loss: [0.00000224 0.00002108]\n",
      "timestep: 201, loss: [0.00000223 0.00001964]\n",
      "timestep: 202, loss: [0.00000223 0.00002056]\n",
      "timestep: 203, loss: [0.00000223 0.00001972]\n",
      "timestep: 204, loss: [0.00000222 0.00001968]\n",
      "timestep: 205, loss: [0.00000222 0.00001926]\n",
      "timestep: 206, loss: [0.00000221 0.00001872]\n",
      "timestep: 207, loss: [0.00000222 0.00001937]\n",
      "timestep: 208, loss: [0.00000221 0.00001813]\n",
      "timestep: 209, loss: [0.00000221 0.00001876]\n",
      "timestep: 210, loss: [0.0000022 0.0000181]\n",
      "timestep: 211, loss: [0.0000022  0.00001791]\n",
      "timestep: 212, loss: [0.0000022  0.00001739]\n",
      "timestep: 213, loss: [0.0000022  0.00001782]\n",
      "timestep: 214, loss: [0.00000219 0.00001707]\n",
      "timestep: 215, loss: [0.00000219 0.00001714]\n",
      "timestep: 216, loss: [0.00000219 0.00001692]\n",
      "timestep: 217, loss: [0.00000218 0.00001636]\n",
      "timestep: 218, loss: [0.00000218 0.00001623]\n",
      "timestep: 219, loss: [0.00000218 0.00001635]\n",
      "timestep: 220, loss: [0.00000218 0.00001588]\n",
      "timestep: 221, loss: [0.00000217 0.00001582]\n",
      "timestep: 222, loss: [0.00000217 0.00001567]\n",
      "timestep: 223, loss: [0.00000216 0.000015  ]\n",
      "timestep: 224, loss: [0.00000217 0.00001503]\n",
      "timestep: 225, loss: [0.00000217 0.00001514]\n",
      "timestep: 226, loss: [0.00000216 0.00001463]\n",
      "timestep: 227, loss: [0.00000216 0.00001464]\n",
      "timestep: 228, loss: [0.00000216 0.00001447]\n",
      "timestep: 229, loss: [0.00000215 0.00001381]\n",
      "timestep: 230, loss: [0.00000215 0.00001429]\n",
      "timestep: 231, loss: [0.00000214 0.00001357]\n",
      "timestep: 232, loss: [0.00000214 0.00001339]\n",
      "timestep: 233, loss: [0.00000215 0.00001374]\n",
      "timestep: 234, loss: [0.00000214 0.00001303]\n",
      "timestep: 235, loss: [0.00000214 0.00001315]\n",
      "timestep: 236, loss: [0.00000214 0.00001293]\n",
      "timestep: 237, loss: [0.00000213 0.00001244]\n",
      "timestep: 238, loss: [0.00000213 0.00001271]\n",
      "timestep: 239, loss: [0.00000213 0.00001218]\n",
      "timestep: 240, loss: [0.00000213 0.00001232]\n",
      "timestep: 241, loss: [0.00000212 0.00001151]\n",
      "timestep: 242, loss: [0.00000213 0.00001232]\n",
      "timestep: 243, loss: [0.00000212 0.00001128]\n",
      "timestep: 244, loss: [0.00000212 0.00001177]\n",
      "timestep: 245, loss: [0.00000211 0.00001122]\n",
      "timestep: 246, loss: [0.00000211 0.00001099]\n",
      "timestep: 247, loss: [0.00000211 0.00001116]\n",
      "timestep: 248, loss: [0.00000211 0.00001075]\n",
      "timestep: 249, loss: [0.00000211 0.00001078]\n",
      "timestep: 250, loss: [0.00000211 0.00001054]\n",
      "timestep: 251, loss: [0.0000021 0.0000101]\n",
      "timestep: 252, loss: [0.00000211 0.00001049]\n",
      "timestep: 253, loss: [0.0000021  0.00000987]\n",
      "timestep: 254, loss: [0.0000021  0.00001009]\n",
      "timestep: 255, loss: [0.00000209 0.00000973]\n",
      "timestep: 256, loss: [0.00000209 0.00000938]\n",
      "timestep: 257, loss: [0.0000021  0.00000976]\n",
      "timestep: 258, loss: [0.00000209 0.00000916]\n",
      "timestep: 259, loss: [0.00000209 0.00000937]\n",
      "timestep: 260, loss: [0.00000208 0.00000903]\n",
      "timestep: 261, loss: [0.00000208 0.00000868]\n",
      "timestep: 262, loss: [0.00000209 0.00000907]\n",
      "timestep: 263, loss: [0.00000208 0.00000849]\n",
      "timestep: 264, loss: [0.00000208 0.0000087 ]\n",
      "timestep: 265, loss: [0.00000208 0.00000835]\n",
      "timestep: 266, loss: [0.00000207 0.00000801]\n",
      "timestep: 267, loss: [0.00000208 0.00000844]\n",
      "timestep: 268, loss: [0.00000207 0.00000783]\n",
      "timestep: 269, loss: [0.00000207 0.00000808]\n",
      "timestep: 270, loss: [0.00000207 0.00000773]\n",
      "timestep: 271, loss: [0.00000206 0.00000739]\n",
      "timestep: 272, loss: [0.00000207 0.00000783]\n",
      "timestep: 273, loss: [0.00000206 0.00000723]\n",
      "timestep: 274, loss: [0.00000206 0.00000749]\n",
      "timestep: 275, loss: [0.00000206 0.00000712]\n",
      "timestep: 276, loss: [0.00000206 0.00000717]\n",
      "timestep: 277, loss: [0.00000205 0.00000663]\n",
      "timestep: 278, loss: [0.00000206 0.00000718]\n",
      "timestep: 279, loss: [0.00000205 0.00000648]\n",
      "timestep: 280, loss: [0.00000206 0.00000684]\n",
      "timestep: 281, loss: [0.00000205 0.00000653]\n",
      "timestep: 282, loss: [0.00000205 0.00000624]\n",
      "timestep: 283, loss: [0.00000205 0.00000657]\n",
      "timestep: 284, loss: [0.00000204 0.00000606]\n",
      "timestep: 285, loss: [0.00000205 0.00000633]\n",
      "timestep: 286, loss: [0.00000204 0.00000592]\n",
      "timestep: 287, loss: [0.00000204 0.00000602]\n",
      "timestep: 288, loss: [0.00000203 0.00000545]\n",
      "timestep: 289, loss: [0.00000204 0.00000612]\n",
      "timestep: 290, loss: [0.00000203 0.00000537]\n",
      "timestep: 291, loss: [0.00000204 0.00000573]\n",
      "timestep: 292, loss: [0.00000203 0.00000542]\n",
      "timestep: 293, loss: [0.00000203 0.00000532]\n",
      "timestep: 294, loss: [0.00000203 0.00000499]\n",
      "timestep: 295, loss: [0.00000203 0.00000543]\n",
      "timestep: 296, loss: [0.00000203 0.00000492]\n",
      "timestep: 297, loss: [0.00000203 0.00000505]\n",
      "timestep: 298, loss: [0.00000203 0.00000499]\n",
      "timestep: 299, loss: [0.00000202 0.00000469]\n",
      "[array([0.00262387, 0.08838134]), array([0.00003928, 0.00044852]), array([0.00000647, 0.00023794]), array([0.00000474, 0.00015042]), array([0.00000424, 0.00014121]), array([0.0000039 , 0.00012148]), array([0.00000362, 0.00010574]), array([0.00000339, 0.00009248]), array([0.0000032 , 0.00008131]), array([0.00000303, 0.00006966]), array([0.00000291, 0.00006317]), array([0.00000281, 0.00005656]), array([0.00000271, 0.00005089]), array([0.00000263, 0.00004576]), array([0.00000254, 0.00004068]), array([0.00000248, 0.00003683]), array([0.00000243, 0.00003364]), array([0.00000236, 0.00002893]), array([0.00000232, 0.00002652]), array([0.00000227, 0.00002295]), array([0.00000224, 0.00002108]), array([0.0000022, 0.0000181]), array([0.00000218, 0.00001588]), array([0.00000215, 0.00001429]), array([0.00000213, 0.00001232]), array([0.00000211, 0.00001054]), array([0.00000208, 0.00000903]), array([0.00000207, 0.00000773]), array([0.00000206, 0.00000684]), array([0.00000203, 0.00000537])]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-247aadf1558e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_criterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "net = SecondNet(vertex_dim, edge_dim, hidden_dim, p = 0.0).double()\n",
    "# net = SimpleNet(vertex_dim, edge_dim, hidden_dim, p = 0.0).double()\n",
    "\n",
    "for data in dataset:\n",
    "    print(data.E)\n",
    "\n",
    "losses = train(net, train_loader, lr = 0.002, iterations = 300, criterion = train_criterion, verbose = True)\n",
    "print(losses[::10])\n",
    "\n",
    "loss = test(net, test_loader, test_criterion)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:prime] *",
   "language": "python",
   "name": "conda-env-prime-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
