{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Numpy 1.16 has memory leak bug  https://github.com/numpy/numpy/issues/13808\n",
      "It is recommended to downgrade to numpy 1.15 or older\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, GATConv, GINConv, global_max_pool, GlobalAttention, GatedGraphConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.utils.convert import from_scipy_sparse_matrix\n",
    "\n",
    "from pyscf import gto, scf, tools, ao2mo\n",
    "\n",
    "\n",
    "import model\n",
    "import train\n",
    "from model import SecondNet, SimpleNet\n",
    "from preprocess import build_graph, build_qm7\n",
    "from train import train, test\n",
    "from hf import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = build_qm7('sto-3g')\n",
    "#Omit first molecule, outlier geometry\n",
    "mols = mols[1:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Encode number of electrons explicitly\n",
    "#TODO: Encode HF features?\n",
    "#TODO: Encode the \"flavor\" of the orbital basis as features as well\n",
    "\n",
    "#TODO: indicate which orbital is first and second in the pair vertices?  This breaks the symmetry,\n",
    "#but we might want this anyway if we want to particularly understand one of the orbitals in the pair\n",
    "#TODO: indicators should be separate features not integer values, stop being lazy\n",
    "#TODO: ACTUALLY USE GCN MODEL\n",
    "#TODO: Fix edge features between single and double, currently those are all zero and graph is disconnected!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/prime/lib/python3.7/site-packages/pyscf/scf/chkfile.py:31: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  with h5py.File(chkfile) as fh5:\n"
     ]
    }
   ],
   "source": [
    "#M: Number of orbitals\n",
    "#N: Number of electrons\n",
    "#F: feature vector length\n",
    "\n",
    "#A is potential matrix: M x M\n",
    "#U is coulumb 4-tensor: M x M x M x M\n",
    "#X is additional orbital feature matrix: M x F_1\n",
    "#Y is additional pairwise orbital feature matrix: M x M x F_2\n",
    "\n",
    "#E is ground state energy\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for mol in mols:\n",
    "    A, U, X, Y, E = get_data(mol, \"AO\", predict_correlation = False)\n",
    "    \n",
    "    ####COMPLETE HACK\n",
    "#     E /= 10.\n",
    "#     np.fill_diagonal(Y[:,:,0], np.diagonal(Y[:,:,0]) / 10.)\n",
    "#     np.fill_diagonal(Y[:,:,1], np.diagonal(Y[:,:,1]) / 10.)\n",
    "    \n",
    "    ####\n",
    "    \n",
    "    M = A.shape[0]\n",
    "    X = np.zeros((M, 1)) #Currently no orbital features\n",
    "                \n",
    "    x, edge_index, edge_attr = build_graph(A, U, X, Y)\n",
    "    \n",
    "    \n",
    "#     print(\"True energy:\\t\\t {}\".format(E))\n",
    "#     print(\"Energy via Trace:\\t {}\".format(np.sum(Y[:,:,0] * Y[:,:,1])))\n",
    "#     print(\"Energy via features:\\t {}\"\n",
    "#           .format(torch.sum(x[:,1] * x[:,2] * (2 - x[:,5])).item()))\n",
    "#     print()\n",
    "\n",
    "        \n",
    "    data = Data(x = x, edge_index = edge_index, edge_attr = edge_attr, y = E)\n",
    "    dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(dataset)\n",
    "\n",
    "split = int(0.8 * len(dataset))\n",
    "train_loader = DataLoader(dataset[:split], batch_size = 6)\n",
    "test_loader = DataLoader(dataset[split:], batch_size = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(model)\n",
    "from model import SecondNet, SimpleNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_dim = dataset[0].x.shape[1]\n",
    "edge_dim = dataset[0].edge_attr.shape[1]\n",
    "hidden_dim = 20\n",
    "\n",
    "train_criterion = nn.MSELoss()\n",
    "test_criterion = nn.L1Loss()\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 0, loss: 544.3663398203461\n",
      "timestep: 1, loss: 438.9710992028379\n",
      "timestep: 2, loss: 398.34177436297233\n",
      "timestep: 3, loss: 397.36148244007467\n",
      "timestep: 4, loss: 383.12508624350534\n",
      "timestep: 5, loss: 375.5904284393231\n",
      "timestep: 6, loss: 367.5097823907338\n",
      "timestep: 7, loss: 357.441342985659\n",
      "timestep: 8, loss: 346.87670452701707\n",
      "timestep: 9, loss: 335.53939396223194\n",
      "timestep: 10, loss: 323.19237847023277\n",
      "timestep: 11, loss: 310.3123538056834\n",
      "timestep: 12, loss: 296.68559388055314\n",
      "timestep: 13, loss: 282.76069046828917\n",
      "timestep: 14, loss: 269.0620824999703\n",
      "timestep: 15, loss: 255.9572529884762\n",
      "timestep: 16, loss: 242.7395061053752\n",
      "timestep: 17, loss: 229.33953419486932\n",
      "timestep: 18, loss: 215.89603858735316\n",
      "timestep: 19, loss: 202.24500105520568\n",
      "timestep: 20, loss: 188.44672205512833\n",
      "timestep: 21, loss: 174.5515461769024\n",
      "timestep: 22, loss: 160.72589605317916\n",
      "timestep: 23, loss: 147.03355199969374\n",
      "timestep: 24, loss: 133.54198215293928\n",
      "timestep: 25, loss: 120.39399394050776\n",
      "timestep: 26, loss: 107.59544590873998\n",
      "timestep: 27, loss: 95.402917070458\n",
      "timestep: 28, loss: 83.4824612125587\n",
      "timestep: 29, loss: 72.13838923719415\n",
      "timestep: 30, loss: 61.52727899393938\n",
      "timestep: 31, loss: 51.75124998886474\n",
      "timestep: 32, loss: 43.30636663677856\n",
      "timestep: 33, loss: 36.42519226003696\n",
      "timestep: 34, loss: 30.204156932908333\n",
      "timestep: 35, loss: 24.859588212234527\n",
      "timestep: 36, loss: 20.50248890330985\n",
      "timestep: 37, loss: 16.96419023682747\n",
      "timestep: 38, loss: 14.363128153295973\n",
      "timestep: 39, loss: 12.217338046766297\n",
      "timestep: 40, loss: 11.195009901822754\n",
      "timestep: 41, loss: 10.501568928303676\n",
      "timestep: 42, loss: 9.598432125758238\n",
      "timestep: 43, loss: 9.141508176038858\n",
      "timestep: 44, loss: 9.308085367623166\n",
      "timestep: 45, loss: 9.105289718454218\n",
      "timestep: 46, loss: 8.019729397749229\n",
      "timestep: 47, loss: 7.021150269415093\n",
      "timestep: 48, loss: 6.816850663020619\n",
      "timestep: 49, loss: 6.947104438976089\n",
      "timestep: 50, loss: 6.824624032648865\n",
      "timestep: 51, loss: 6.370354762376096\n",
      "timestep: 52, loss: 6.0092022361581146\n",
      "timestep: 53, loss: 5.915045966496791\n",
      "timestep: 54, loss: 5.9405487324870085\n",
      "timestep: 55, loss: 5.840707562128895\n",
      "timestep: 56, loss: 5.620090292159019\n",
      "timestep: 57, loss: 5.443325440326865\n",
      "timestep: 58, loss: 5.381561068851318\n",
      "timestep: 59, loss: 5.398444518242896\n",
      "timestep: 60, loss: 5.405495559792976\n",
      "timestep: 61, loss: 5.261129241374204\n",
      "timestep: 62, loss: 5.113844306854815\n",
      "timestep: 63, loss: 5.048957527963963\n",
      "timestep: 64, loss: 4.994163230226278\n",
      "timestep: 65, loss: 4.920830865927713\n",
      "timestep: 66, loss: 4.8820559428316885\n",
      "timestep: 67, loss: 4.85483464824196\n",
      "timestep: 68, loss: 4.833821827320258\n",
      "timestep: 69, loss: 4.835710684199837\n",
      "timestep: 70, loss: 4.822565872250168\n",
      "timestep: 71, loss: 4.782199986062773\n",
      "timestep: 72, loss: 4.741502884796772\n",
      "timestep: 73, loss: 4.708708592870575\n",
      "timestep: 74, loss: 4.67153491172765\n",
      "timestep: 75, loss: 4.637678239874386\n",
      "timestep: 76, loss: 4.629649249949103\n",
      "timestep: 77, loss: 4.566414284106004\n",
      "timestep: 78, loss: 4.524341239599398\n",
      "timestep: 79, loss: 4.5152745209990455\n",
      "timestep: 80, loss: 4.526994884341434\n",
      "timestep: 81, loss: 4.512357025293693\n",
      "timestep: 82, loss: 4.457233006638664\n",
      "timestep: 83, loss: 4.394939910905155\n",
      "timestep: 84, loss: 4.391095250922891\n",
      "timestep: 85, loss: 4.420034175134064\n",
      "timestep: 86, loss: 4.386946478998291\n",
      "timestep: 87, loss: 4.34019806995876\n",
      "timestep: 88, loss: 4.324090910876909\n",
      "timestep: 89, loss: 4.306192679930636\n",
      "timestep: 90, loss: 4.285968147080569\n",
      "timestep: 91, loss: 4.251056032491818\n",
      "timestep: 92, loss: 4.2383356153435665\n",
      "timestep: 93, loss: 4.222142059735749\n",
      "timestep: 94, loss: 4.171511691329767\n",
      "timestep: 95, loss: 4.150287334415564\n",
      "timestep: 96, loss: 4.099841317532843\n",
      "timestep: 97, loss: 4.073523676208757\n",
      "timestep: 98, loss: 4.0621924632266655\n",
      "timestep: 99, loss: 4.068408899848674\n",
      "timestep: 100, loss: 4.055167478450598\n",
      "timestep: 101, loss: 4.0387763980701115\n",
      "timestep: 102, loss: 4.017691360076033\n",
      "timestep: 103, loss: 4.016630930877616\n",
      "timestep: 104, loss: 3.9979473463225417\n",
      "timestep: 105, loss: 3.990978173194388\n",
      "timestep: 106, loss: 3.9604697307301184\n",
      "timestep: 107, loss: 3.925957445781066\n",
      "timestep: 108, loss: 3.9065716597331903\n",
      "timestep: 109, loss: 3.848215938161701\n",
      "timestep: 110, loss: 3.8029284281787197\n",
      "timestep: 111, loss: 3.800239571064956\n",
      "timestep: 112, loss: 3.810800110929672\n",
      "timestep: 113, loss: 3.8133206891831577\n",
      "timestep: 114, loss: 3.811664212688129\n",
      "timestep: 115, loss: 3.8244594347826304\n",
      "timestep: 116, loss: 3.8282296256210597\n",
      "timestep: 117, loss: 3.837703297018629\n",
      "timestep: 118, loss: 3.822128047784732\n",
      "timestep: 119, loss: 3.8080514598619755\n",
      "timestep: 120, loss: 3.8041948848119578\n",
      "timestep: 121, loss: 3.7906068336720864\n",
      "timestep: 122, loss: 3.78169974406578\n",
      "timestep: 123, loss: 3.777055287659323\n",
      "timestep: 124, loss: 3.7697848111866334\n",
      "timestep: 125, loss: 3.766534865856143\n",
      "timestep: 126, loss: 3.7637606027265647\n",
      "timestep: 127, loss: 3.7431572401764437\n",
      "timestep: 128, loss: 3.7321353787421443\n",
      "timestep: 129, loss: 3.719111542095782\n",
      "timestep: 130, loss: 3.725584014654288\n",
      "timestep: 131, loss: 3.723518028996119\n",
      "timestep: 132, loss: 3.715770793343172\n",
      "timestep: 133, loss: 3.682304165791656\n",
      "timestep: 134, loss: 3.6506168453373333\n",
      "timestep: 135, loss: 3.629678216029657\n",
      "timestep: 136, loss: 3.6202696880739698\n",
      "timestep: 137, loss: 3.6256479358291216\n",
      "timestep: 138, loss: 3.6289342690489033\n",
      "timestep: 139, loss: 3.623921521102105\n",
      "timestep: 140, loss: 3.625251269081432\n",
      "timestep: 141, loss: 3.6360106147390954\n",
      "timestep: 142, loss: 3.653571994743848\n",
      "timestep: 143, loss: 3.6471183464455446\n",
      "timestep: 144, loss: 3.6438430056882707\n",
      "timestep: 145, loss: 3.6408766628510474\n",
      "timestep: 146, loss: 3.6443703140571286\n",
      "timestep: 147, loss: 3.6540293845418996\n",
      "timestep: 148, loss: 3.6468373895578283\n",
      "timestep: 149, loss: 3.644136416245622\n",
      "timestep: 150, loss: 3.6558406503539107\n",
      "timestep: 151, loss: 3.6473744206354484\n",
      "timestep: 152, loss: 3.639850204988789\n",
      "timestep: 153, loss: 3.6435293932367965\n",
      "timestep: 154, loss: 3.6403101971075715\n",
      "timestep: 155, loss: 3.6376860889305163\n",
      "timestep: 156, loss: 3.6265580780405737\n",
      "timestep: 157, loss: 3.608945655369275\n",
      "timestep: 158, loss: 3.6059806688797917\n",
      "timestep: 159, loss: 3.6093476262485003\n",
      "timestep: 160, loss: 3.6119282440120304\n",
      "timestep: 161, loss: 3.6078803524495133\n",
      "timestep: 162, loss: 3.598340395104943\n",
      "timestep: 163, loss: 3.5845699406936915\n",
      "timestep: 164, loss: 3.582289489052623\n",
      "timestep: 165, loss: 3.5858713182334707\n",
      "timestep: 166, loss: 3.5771471914211572\n",
      "timestep: 167, loss: 3.568148179545449\n",
      "timestep: 168, loss: 3.557607245989627\n",
      "timestep: 169, loss: 3.5643113967687245\n",
      "timestep: 170, loss: 3.5729319346997896\n",
      "timestep: 171, loss: 3.568050305036056\n",
      "timestep: 172, loss: 3.5692414046791545\n",
      "timestep: 173, loss: 3.5647823252270996\n",
      "timestep: 174, loss: 3.5701101295334463\n",
      "timestep: 175, loss: 3.5680813678930132\n",
      "timestep: 176, loss: 3.5652288391767737\n",
      "timestep: 177, loss: 3.561750431674383\n",
      "timestep: 178, loss: 3.5655762007212157\n",
      "timestep: 179, loss: 3.565040441196187\n",
      "timestep: 180, loss: 3.556982386205365\n",
      "timestep: 181, loss: 3.557031255822374\n",
      "timestep: 182, loss: 3.5575792552516443\n",
      "timestep: 183, loss: 3.548298891803441\n",
      "timestep: 184, loss: 3.5457020182044037\n",
      "timestep: 185, loss: 3.541391622614637\n",
      "timestep: 186, loss: 3.541147265658775\n",
      "timestep: 187, loss: 3.5454268509995033\n",
      "timestep: 188, loss: 3.5406014561210286\n",
      "timestep: 189, loss: 3.528352497167033\n",
      "timestep: 190, loss: 3.520808685512046\n",
      "timestep: 191, loss: 3.5170150037763337\n",
      "timestep: 192, loss: 3.506518763745451\n",
      "timestep: 193, loss: 3.4969250074027314\n",
      "timestep: 194, loss: 3.4994777160046273\n",
      "timestep: 195, loss: 3.494972052400827\n",
      "timestep: 196, loss: 3.489490765123442\n",
      "timestep: 197, loss: 3.48638752826848\n",
      "timestep: 198, loss: 3.4780603324217996\n",
      "timestep: 199, loss: 3.477161946210403\n",
      "timestep: 200, loss: 3.4747455463912518\n",
      "timestep: 201, loss: 3.4668939186483128\n",
      "timestep: 202, loss: 3.4606409682509733\n",
      "timestep: 203, loss: 3.465772640967286\n",
      "timestep: 204, loss: 3.4596577260151773\n",
      "timestep: 205, loss: 3.445738812379245\n",
      "timestep: 206, loss: 3.443754754870602\n",
      "timestep: 207, loss: 3.4396417743529337\n",
      "timestep: 208, loss: 3.4449027598400703\n",
      "timestep: 209, loss: 3.442048174069876\n",
      "timestep: 210, loss: 3.4396866640301074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 211, loss: 3.4295416328618895\n",
      "timestep: 212, loss: 3.425114228219573\n",
      "timestep: 213, loss: 3.4156195777662206\n",
      "timestep: 214, loss: 3.4095071319604027\n",
      "timestep: 215, loss: 3.3984357770892397\n",
      "timestep: 216, loss: 3.3921780706592513\n",
      "timestep: 217, loss: 3.3810928156711637\n",
      "timestep: 218, loss: 3.3820990181055595\n",
      "timestep: 219, loss: 3.3881337083841796\n",
      "timestep: 220, loss: 3.3796175946916964\n",
      "timestep: 221, loss: 3.367845414320669\n",
      "timestep: 222, loss: 3.3639562049036793\n",
      "timestep: 223, loss: 3.3571539702108426\n",
      "timestep: 224, loss: 3.35638481857158\n",
      "timestep: 225, loss: 3.354843413222828\n",
      "timestep: 226, loss: 3.346843026694712\n",
      "timestep: 227, loss: 3.3325402312359156\n",
      "timestep: 228, loss: 3.3222545999798183\n",
      "timestep: 229, loss: 3.3262286265977097\n",
      "timestep: 230, loss: 3.3285153461029977\n",
      "timestep: 231, loss: 3.3122055440481772\n",
      "timestep: 232, loss: 3.3019264648395463\n",
      "timestep: 233, loss: 3.2899189467027323\n",
      "timestep: 234, loss: 3.279449690255843\n",
      "timestep: 235, loss: 3.2712527491359107\n",
      "timestep: 236, loss: 3.2633405620182345\n",
      "timestep: 237, loss: 3.252380119741687\n",
      "timestep: 238, loss: 3.240516892660982\n",
      "timestep: 239, loss: 3.238080352995284\n",
      "timestep: 240, loss: 3.2334941254001035\n",
      "timestep: 241, loss: 3.230918041732864\n",
      "timestep: 242, loss: 3.226356570609545\n",
      "timestep: 243, loss: 3.2171330999789336\n",
      "timestep: 244, loss: 3.221010041570048\n",
      "timestep: 245, loss: 3.2157426880149873\n",
      "timestep: 246, loss: 3.2076460166864176\n",
      "timestep: 247, loss: 3.1977989661601143\n",
      "timestep: 248, loss: 3.1885287484663123\n",
      "timestep: 249, loss: 3.183956034613959\n",
      "timestep: 250, loss: 3.179861926224426\n",
      "timestep: 251, loss: 3.1742811541693925\n",
      "timestep: 252, loss: 3.1620305532176327\n",
      "timestep: 253, loss: 3.1489016196818014\n",
      "timestep: 254, loss: 3.1366820716690356\n",
      "timestep: 255, loss: 3.1311080070386805\n",
      "timestep: 256, loss: 3.123189213027024\n",
      "timestep: 257, loss: 3.116884441775957\n",
      "timestep: 258, loss: 3.11176522394783\n",
      "timestep: 259, loss: 3.1057839342783216\n",
      "timestep: 260, loss: 3.0926932134779834\n",
      "timestep: 261, loss: 3.0864188134755435\n",
      "timestep: 262, loss: 3.0828016761053543\n",
      "timestep: 263, loss: 3.0866881546803056\n",
      "timestep: 264, loss: 3.0800874415953006\n",
      "timestep: 265, loss: 3.0755070925947137\n",
      "timestep: 266, loss: 3.0723465039946065\n",
      "timestep: 267, loss: 3.0687125869700185\n",
      "timestep: 268, loss: 3.0628753527765884\n",
      "timestep: 269, loss: 3.0586107195342382\n",
      "timestep: 270, loss: 3.058591679108858\n",
      "timestep: 271, loss: 3.0602994005108886\n",
      "timestep: 272, loss: 3.0527155501426595\n",
      "timestep: 273, loss: 3.037556729704306\n",
      "timestep: 274, loss: 3.024191190325837\n",
      "timestep: 275, loss: 3.016520361044141\n",
      "timestep: 276, loss: 3.0144064678444593\n",
      "timestep: 277, loss: 3.0110833838945172\n",
      "timestep: 278, loss: 3.004156610728988\n",
      "timestep: 279, loss: 2.9996450388180858\n",
      "timestep: 280, loss: 2.9972084354765967\n",
      "timestep: 281, loss: 2.9949536720226404\n",
      "timestep: 282, loss: 2.9907380747779184\n",
      "timestep: 283, loss: 2.9873463142963614\n",
      "timestep: 284, loss: 2.9870416212730033\n",
      "timestep: 285, loss: 2.984338302370013\n",
      "timestep: 286, loss: 2.9839392388952404\n",
      "timestep: 287, loss: 2.98395316985427\n",
      "timestep: 288, loss: 2.9781486680135725\n",
      "timestep: 289, loss: 2.9747216869470594\n",
      "timestep: 290, loss: 2.968773987198952\n",
      "timestep: 291, loss: 2.9647567163374346\n",
      "timestep: 292, loss: 2.9628799809676405\n",
      "timestep: 293, loss: 2.9606434621970767\n",
      "timestep: 294, loss: 2.957261540501845\n",
      "timestep: 295, loss: 2.9506062974809057\n",
      "timestep: 296, loss: 2.943543647205594\n",
      "timestep: 297, loss: 2.937396741866405\n",
      "timestep: 298, loss: 2.930133327174637\n",
      "timestep: 299, loss: 2.925490191413692\n",
      "timestep: 300, loss: 2.9224385323044486\n",
      "timestep: 301, loss: 2.9132950373365882\n",
      "timestep: 302, loss: 2.9008692752829077\n",
      "timestep: 303, loss: 2.891869189873269\n",
      "timestep: 304, loss: 2.8827820075208965\n",
      "timestep: 305, loss: 2.870032733620042\n",
      "timestep: 306, loss: 2.8596783675167923\n",
      "timestep: 307, loss: 2.8536458639060927\n",
      "timestep: 308, loss: 2.8453849904515343\n",
      "timestep: 309, loss: 2.832598103463009\n",
      "timestep: 310, loss: 2.8190603516800063\n",
      "timestep: 311, loss: 2.815190617262173\n",
      "timestep: 312, loss: 2.8060835419792522\n",
      "timestep: 313, loss: 2.799436561474222\n",
      "timestep: 314, loss: 2.78846231716784\n",
      "timestep: 315, loss: 2.779032989141636\n",
      "timestep: 316, loss: 2.7621666148732396\n",
      "timestep: 317, loss: 2.749539926711081\n",
      "timestep: 318, loss: 2.7420923432757345\n",
      "timestep: 319, loss: 2.7413532524860003\n",
      "timestep: 320, loss: 2.737410248691464\n",
      "timestep: 321, loss: 2.7343955674798974\n",
      "timestep: 322, loss: 2.734004236486177\n",
      "timestep: 323, loss: 2.7327100886506814\n",
      "timestep: 324, loss: 2.7256172339218434\n",
      "timestep: 325, loss: 2.7176182777708964\n",
      "timestep: 326, loss: 2.718119753454413\n",
      "timestep: 327, loss: 2.712752377129118\n",
      "timestep: 328, loss: 2.7086481823304083\n",
      "timestep: 329, loss: 2.712499171951835\n",
      "timestep: 330, loss: 2.717401117902955\n",
      "timestep: 331, loss: 2.7217474153510994\n",
      "timestep: 332, loss: 2.7253943091281774\n",
      "timestep: 333, loss: 2.724810217584478\n",
      "timestep: 334, loss: 2.7201637599946324\n",
      "timestep: 335, loss: 2.720363909432021\n",
      "timestep: 336, loss: 2.7223493423743945\n",
      "timestep: 337, loss: 2.7266911957675606\n",
      "timestep: 338, loss: 2.726935477336841\n",
      "timestep: 339, loss: 2.728711590865027\n",
      "timestep: 340, loss: 2.725517508313966\n",
      "timestep: 341, loss: 2.724559368545239\n",
      "timestep: 342, loss: 2.720541808744989\n",
      "timestep: 343, loss: 2.7198726397019604\n",
      "timestep: 344, loss: 2.7215693109377157\n",
      "timestep: 345, loss: 2.723685915210874\n",
      "timestep: 346, loss: 2.723308694693705\n",
      "timestep: 347, loss: 2.7269994742027754\n",
      "timestep: 348, loss: 2.725363331755472\n",
      "timestep: 349, loss: 2.7211957869888557\n",
      "timestep: 350, loss: 2.7158642877786647\n",
      "timestep: 351, loss: 2.712650915523781\n",
      "timestep: 352, loss: 2.710002738653783\n",
      "timestep: 353, loss: 2.707416337621619\n",
      "timestep: 354, loss: 2.706410636855069\n",
      "timestep: 355, loss: 2.7069800290042463\n",
      "timestep: 356, loss: 2.7035485058056055\n",
      "timestep: 357, loss: 2.6950550578400105\n",
      "timestep: 358, loss: 2.683875878962877\n",
      "timestep: 359, loss: 2.6769430639696363\n",
      "timestep: 360, loss: 2.671229422860358\n",
      "timestep: 361, loss: 2.667686347278443\n",
      "timestep: 362, loss: 2.6634203496393725\n",
      "timestep: 363, loss: 2.664968059353919\n",
      "timestep: 364, loss: 2.6653956537437495\n",
      "timestep: 365, loss: 2.664176681193278\n",
      "timestep: 366, loss: 2.6612155454893887\n",
      "timestep: 367, loss: 2.6557160512828126\n",
      "timestep: 368, loss: 2.6503577479160714\n",
      "timestep: 369, loss: 2.6409759511825075\n",
      "timestep: 370, loss: 2.6288853427721253\n",
      "timestep: 371, loss: 2.6199835402131693\n",
      "timestep: 372, loss: 2.6170048575735687\n",
      "timestep: 373, loss: 2.6109155074300867\n",
      "timestep: 374, loss: 2.6048869147201374\n",
      "timestep: 375, loss: 2.5968242187793735\n",
      "timestep: 376, loss: 2.58873054531367\n",
      "timestep: 377, loss: 2.577615137953457\n",
      "timestep: 378, loss: 2.566413397502678\n",
      "timestep: 379, loss: 2.5619003038041606\n",
      "timestep: 380, loss: 2.556790168248595\n",
      "timestep: 381, loss: 2.548998507201046\n",
      "timestep: 382, loss: 2.5431619432144994\n",
      "timestep: 383, loss: 2.5414850381517846\n",
      "timestep: 384, loss: 2.5404018450691885\n",
      "timestep: 385, loss: 2.5448627102270223\n",
      "timestep: 386, loss: 2.5411209297817874\n",
      "timestep: 387, loss: 2.526626142004027\n",
      "timestep: 388, loss: 2.5115368255860795\n",
      "timestep: 389, loss: 2.4936348374182113\n",
      "timestep: 390, loss: 2.480007764774136\n",
      "timestep: 391, loss: 2.4542405760153976\n",
      "timestep: 392, loss: 2.4138337322743926\n",
      "timestep: 393, loss: 2.3678402086893335\n",
      "timestep: 394, loss: 2.330309986165491\n",
      "timestep: 395, loss: 2.310941774336114\n",
      "timestep: 396, loss: 2.315476762666396\n",
      "timestep: 397, loss: 2.3319584363592356\n",
      "timestep: 398, loss: 2.354673869626849\n",
      "timestep: 399, loss: 2.37738273264705\n",
      "timestep: 400, loss: 2.396572273108721\n",
      "timestep: 401, loss: 2.4092646361162435\n",
      "timestep: 402, loss: 2.4254073955961384\n",
      "timestep: 403, loss: 2.4341653132982852\n",
      "timestep: 404, loss: 2.442943589140024\n",
      "timestep: 405, loss: 2.4523270211168717\n",
      "timestep: 406, loss: 2.4599262854088755\n",
      "timestep: 407, loss: 2.464298161719362\n",
      "timestep: 408, loss: 2.4673169125099643\n",
      "timestep: 409, loss: 2.471326553094016\n",
      "timestep: 410, loss: 2.476074403859246\n",
      "timestep: 411, loss: 2.479763039610417\n",
      "timestep: 412, loss: 2.4795313499082736\n",
      "timestep: 413, loss: 2.476572242036933\n",
      "timestep: 414, loss: 2.4638887822364706\n",
      "timestep: 415, loss: 2.4610186279403155\n",
      "timestep: 416, loss: 2.449249833263058\n",
      "timestep: 417, loss: 2.4310731971910737\n",
      "timestep: 418, loss: 2.4140560136148657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 419, loss: 2.3883590129460868\n",
      "timestep: 420, loss: 2.365374553808783\n",
      "timestep: 421, loss: 2.3481122685700857\n",
      "timestep: 422, loss: 2.3458890867973192\n",
      "timestep: 423, loss: 2.3422788306697253\n",
      "timestep: 424, loss: 2.334234084040452\n",
      "timestep: 425, loss: 2.326072035895599\n",
      "timestep: 426, loss: 2.3175941905234296\n",
      "timestep: 427, loss: 2.3054513727960417\n",
      "timestep: 428, loss: 2.3049565309003133\n",
      "timestep: 429, loss: 2.3049980670678347\n",
      "timestep: 430, loss: 2.3014326626162473\n",
      "timestep: 431, loss: 2.2949616454260844\n",
      "timestep: 432, loss: 2.28730545860746\n",
      "timestep: 433, loss: 2.2743349350540787\n",
      "timestep: 434, loss: 2.2586836840660953\n",
      "timestep: 435, loss: 2.2514648764046314\n",
      "timestep: 436, loss: 2.2543591636011295\n",
      "timestep: 437, loss: 2.2530551394977754\n",
      "timestep: 438, loss: 2.2521231939841595\n",
      "timestep: 439, loss: 2.250123012530189\n",
      "timestep: 440, loss: 2.2463159323247344\n",
      "timestep: 441, loss: 2.2391597191144643\n",
      "timestep: 442, loss: 2.2432041370690574\n",
      "timestep: 443, loss: 2.2460755303451716\n",
      "timestep: 444, loss: 2.2533945971178735\n",
      "timestep: 445, loss: 2.267160382725153\n",
      "timestep: 446, loss: 2.2782641057645194\n",
      "timestep: 447, loss: 2.288485159702836\n",
      "timestep: 448, loss: 2.297561992092035\n",
      "timestep: 449, loss: 2.305123392514432\n",
      "timestep: 450, loss: 2.3084411104256097\n",
      "timestep: 451, loss: 2.316697757382895\n",
      "timestep: 452, loss: 2.316560294051274\n",
      "timestep: 453, loss: 2.3153747850462634\n",
      "timestep: 454, loss: 2.312756119912629\n",
      "timestep: 455, loss: 2.311709886143834\n",
      "timestep: 456, loss: 2.3050217440408325\n",
      "timestep: 457, loss: 2.296514718853921\n",
      "timestep: 458, loss: 2.291876142321005\n",
      "timestep: 459, loss: 2.2913601170977795\n",
      "timestep: 460, loss: 2.2908837080706346\n",
      "timestep: 461, loss: 2.287430637702202\n",
      "timestep: 462, loss: 2.2867514461229956\n",
      "timestep: 463, loss: 2.290956783169639\n",
      "timestep: 464, loss: 2.2978105209787634\n",
      "timestep: 465, loss: 2.303463060173041\n",
      "timestep: 466, loss: 2.3047364474474246\n",
      "timestep: 467, loss: 2.3015547940060936\n",
      "timestep: 468, loss: 2.297425088191806\n",
      "timestep: 469, loss: 2.291855033800976\n",
      "timestep: 470, loss: 2.2824482383181337\n",
      "timestep: 471, loss: 2.2725333179017646\n",
      "timestep: 472, loss: 2.2638866767876475\n",
      "timestep: 473, loss: 2.255547447417665\n",
      "timestep: 474, loss: 2.248316048632942\n",
      "timestep: 475, loss: 2.23103710940368\n",
      "timestep: 476, loss: 2.213842921687383\n",
      "timestep: 477, loss: 2.2003985203913885\n",
      "timestep: 478, loss: 2.189773267406704\n",
      "timestep: 479, loss: 2.1820763432073127\n",
      "timestep: 480, loss: 2.1796790944362225\n",
      "timestep: 481, loss: 2.175505066858081\n",
      "timestep: 482, loss: 2.1612773006712573\n",
      "timestep: 483, loss: 2.1044290064026514\n",
      "timestep: 484, loss: 2.0117119130391643\n",
      "timestep: 485, loss: 1.8975423730691652\n",
      "timestep: 486, loss: 1.8038396527425602\n",
      "timestep: 487, loss: 1.7592267255043124\n",
      "timestep: 488, loss: 1.7611505860008714\n",
      "timestep: 489, loss: 1.7937923631996506\n",
      "timestep: 490, loss: 1.8438446137880986\n",
      "timestep: 491, loss: 1.9035514230854609\n",
      "timestep: 492, loss: 1.9592972617913722\n",
      "timestep: 493, loss: 2.00473008251741\n",
      "timestep: 494, loss: 2.04226998643801\n",
      "timestep: 495, loss: 2.073477563978169\n",
      "timestep: 496, loss: 2.098802393537925\n",
      "timestep: 497, loss: 2.1162866067996338\n",
      "timestep: 498, loss: 2.1273314428298375\n",
      "timestep: 499, loss: 2.136095759889116\n",
      "timestep: 500, loss: 2.1413841933556665\n",
      "timestep: 501, loss: 2.1360185843955337\n",
      "timestep: 502, loss: 2.1284100386768965\n",
      "timestep: 503, loss: 2.119733221884408\n",
      "timestep: 504, loss: 2.1061908011529704\n",
      "timestep: 505, loss: 2.089001904361263\n",
      "timestep: 506, loss: 2.077046460131093\n",
      "timestep: 507, loss: 2.07239677128571\n",
      "timestep: 508, loss: 2.0694926159688865\n",
      "timestep: 509, loss: 2.0709452830745008\n",
      "timestep: 510, loss: 2.0840607778687774\n",
      "timestep: 511, loss: 2.0915645896375747\n",
      "timestep: 512, loss: 2.1028496397465593\n",
      "timestep: 513, loss: 2.1140503543261824\n",
      "timestep: 514, loss: 2.117105593114661\n",
      "timestep: 515, loss: 2.1134865850180193\n",
      "timestep: 516, loss: 2.111464381673296\n",
      "timestep: 517, loss: 2.1074938528886804\n",
      "timestep: 518, loss: 2.1020726758603754\n",
      "timestep: 519, loss: 2.098583537444284\n",
      "timestep: 520, loss: 2.09532021710073\n",
      "timestep: 521, loss: 2.0977646926074067\n",
      "timestep: 522, loss: 2.1016760064492774\n",
      "timestep: 523, loss: 2.110102245875106\n",
      "timestep: 524, loss: 2.1149537354102037\n",
      "timestep: 525, loss: 2.1194563234913746\n",
      "timestep: 526, loss: 2.1195043286326136\n",
      "timestep: 527, loss: 2.1214752797077674\n",
      "timestep: 528, loss: 2.1213770159632137\n",
      "timestep: 529, loss: 2.1205110737354818\n",
      "timestep: 530, loss: 2.123414180056135\n",
      "timestep: 531, loss: 2.1194933300903895\n",
      "timestep: 532, loss: 2.1169924509444598\n",
      "timestep: 533, loss: 2.1140130269619792\n",
      "timestep: 534, loss: 2.1084177328703952\n",
      "timestep: 535, loss: 2.10522083466587\n",
      "timestep: 536, loss: 2.100407912730737\n",
      "timestep: 537, loss: 2.096634206339121\n",
      "timestep: 538, loss: 2.091445106096364\n",
      "timestep: 539, loss: 2.081674872875618\n",
      "timestep: 540, loss: 2.0765847689149743\n",
      "timestep: 541, loss: 2.0698500229832635\n",
      "timestep: 542, loss: 2.065722524503387\n",
      "timestep: 543, loss: 2.0653549979542727\n",
      "timestep: 544, loss: 2.063286324922699\n",
      "timestep: 545, loss: 2.063916809653227\n",
      "timestep: 546, loss: 2.0611843648860417\n",
      "timestep: 547, loss: 2.060770636536241\n",
      "timestep: 548, loss: 2.0573595674074086\n",
      "timestep: 549, loss: 2.05446989371106\n",
      "timestep: 550, loss: 2.048414630274327\n",
      "timestep: 551, loss: 2.0474300946317885\n",
      "timestep: 552, loss: 2.044891277788553\n",
      "timestep: 553, loss: 2.0435739830686455\n",
      "timestep: 554, loss: 2.0406163252962273\n",
      "timestep: 555, loss: 2.042781652234493\n",
      "timestep: 556, loss: 2.044657904746614\n",
      "timestep: 557, loss: 2.047391797032343\n",
      "timestep: 558, loss: 2.047609638775022\n",
      "timestep: 559, loss: 2.0483149228211386\n",
      "timestep: 560, loss: 2.0439566634762465\n",
      "timestep: 561, loss: 2.0387636030790732\n",
      "timestep: 562, loss: 2.0329514435129745\n",
      "timestep: 563, loss: 2.0274122933229277\n",
      "timestep: 564, loss: 2.017906830731455\n",
      "timestep: 565, loss: 2.010903537185901\n",
      "timestep: 566, loss: 2.006809790767871\n",
      "timestep: 567, loss: 2.0077316673752446\n",
      "timestep: 568, loss: 2.00514825621124\n",
      "timestep: 569, loss: 2.006155412208559\n",
      "timestep: 570, loss: 2.0063555854859856\n",
      "timestep: 571, loss: 2.0106428670964376\n",
      "timestep: 572, loss: 2.010282673658465\n",
      "timestep: 573, loss: 2.0075326118334393\n",
      "timestep: 574, loss: 2.0016041151440813\n",
      "timestep: 575, loss: 1.9951356593271554\n",
      "timestep: 576, loss: 1.9873263340011038\n",
      "timestep: 577, loss: 1.9751141153199796\n",
      "timestep: 578, loss: 1.9581212187220594\n",
      "timestep: 579, loss: 1.9397563750492546\n",
      "timestep: 580, loss: 1.9177114099788994\n",
      "timestep: 581, loss: 1.8974973087680547\n",
      "timestep: 582, loss: 1.8797851193667592\n",
      "timestep: 583, loss: 1.868005555295958\n",
      "timestep: 584, loss: 1.8593917169095795\n",
      "timestep: 585, loss: 1.8566942765155707\n",
      "timestep: 586, loss: 1.8492406046217211\n",
      "timestep: 587, loss: 1.8387402461267115\n",
      "timestep: 588, loss: 1.8155242983661781\n",
      "timestep: 589, loss: 1.7964143512006336\n",
      "timestep: 590, loss: 1.779397611664014\n",
      "timestep: 591, loss: 1.7708240060831648\n",
      "timestep: 592, loss: 1.7766973549002776\n",
      "timestep: 593, loss: 1.7863174519303393\n",
      "timestep: 594, loss: 1.8020541112878121\n",
      "timestep: 595, loss: 1.8246824356206692\n",
      "timestep: 596, loss: 1.8399733181448044\n",
      "timestep: 597, loss: 1.8542376827177747\n",
      "timestep: 598, loss: 1.8705224635737674\n",
      "timestep: 599, loss: 1.8733086508797612\n",
      "timestep: 600, loss: 1.8708125184199458\n",
      "timestep: 601, loss: 1.8700016397156545\n",
      "timestep: 602, loss: 1.8655132139311852\n",
      "timestep: 603, loss: 1.8625792319991645\n",
      "timestep: 604, loss: 1.8627713188004762\n",
      "timestep: 605, loss: 1.8635223732233888\n",
      "timestep: 606, loss: 1.868586200360796\n",
      "timestep: 607, loss: 1.8690430669775182\n",
      "timestep: 608, loss: 1.867099898057499\n",
      "timestep: 609, loss: 1.8633801847338702\n",
      "timestep: 610, loss: 1.8602836270988015\n",
      "timestep: 611, loss: 1.857745115350331\n",
      "timestep: 612, loss: 1.8578730394319978\n",
      "timestep: 613, loss: 1.8608677144252872\n",
      "timestep: 614, loss: 1.8684974422871292\n",
      "timestep: 615, loss: 1.875154707883845\n",
      "timestep: 616, loss: 1.8873666031735767\n",
      "timestep: 617, loss: 1.8938956931388242\n",
      "timestep: 618, loss: 1.8981401656846446\n",
      "timestep: 619, loss: 1.9011318569269\n",
      "timestep: 620, loss: 1.9060195383154712\n",
      "timestep: 621, loss: 1.9054660518830175\n",
      "timestep: 622, loss: 1.9077512584193392\n",
      "timestep: 623, loss: 1.9128506143062916\n",
      "timestep: 624, loss: 1.9118383360264133\n",
      "timestep: 625, loss: 1.9119292174648286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 626, loss: 1.9093687939193664\n",
      "timestep: 627, loss: 1.8976024029363747\n",
      "timestep: 628, loss: 1.8802138613236223\n",
      "timestep: 629, loss: 1.8596455977570405\n",
      "timestep: 630, loss: 1.8367326075531694\n",
      "timestep: 631, loss: 1.7964528936421984\n",
      "timestep: 632, loss: 1.7537138957565253\n",
      "timestep: 633, loss: 1.6987890972995698\n",
      "timestep: 634, loss: 1.6319603578686526\n",
      "timestep: 635, loss: 1.5709420859494438\n",
      "timestep: 636, loss: 1.4999501135817135\n",
      "timestep: 637, loss: 1.4423083378890404\n",
      "timestep: 638, loss: 1.4040092401293638\n",
      "timestep: 639, loss: 1.3917035341724988\n",
      "timestep: 640, loss: 1.3909548271319057\n",
      "timestep: 641, loss: 1.404255100945402\n",
      "timestep: 642, loss: 1.434892335519172\n",
      "timestep: 643, loss: 1.4727687570093548\n",
      "timestep: 644, loss: 1.5283399147894756\n",
      "timestep: 645, loss: 1.5882407626559647\n",
      "timestep: 646, loss: 1.6512585702195386\n",
      "timestep: 647, loss: 1.7183594870905727\n",
      "timestep: 648, loss: 1.7777109137610603\n",
      "timestep: 649, loss: 1.830453665585572\n",
      "timestep: 650, loss: 1.872860957095867\n",
      "timestep: 651, loss: 1.9004520236188696\n",
      "timestep: 652, loss: 1.9196245694158252\n",
      "timestep: 653, loss: 1.921308052877515\n",
      "timestep: 654, loss: 1.9163488979955956\n",
      "timestep: 655, loss: 1.8971053930320323\n",
      "timestep: 656, loss: 1.870505617110988\n",
      "timestep: 657, loss: 1.8391593600473193\n",
      "timestep: 658, loss: 1.8042236951156907\n",
      "timestep: 659, loss: 1.776475846061853\n",
      "timestep: 660, loss: 1.7434749301366366\n",
      "timestep: 661, loss: 1.7203837272452964\n",
      "timestep: 662, loss: 1.7002728386149633\n",
      "timestep: 663, loss: 1.6946211105601838\n",
      "timestep: 664, loss: 1.6931921147813371\n",
      "timestep: 665, loss: 1.695673202779004\n",
      "timestep: 666, loss: 1.6994564561271084\n",
      "timestep: 667, loss: 1.711284584621363\n",
      "timestep: 668, loss: 1.7210936498911558\n",
      "timestep: 669, loss: 1.7364010705937063\n",
      "timestep: 670, loss: 1.7459246892707734\n",
      "timestep: 671, loss: 1.7577555721414144\n",
      "timestep: 672, loss: 1.7667563725859692\n",
      "timestep: 673, loss: 1.7657436290608373\n",
      "timestep: 674, loss: 1.7506218004496865\n",
      "timestep: 675, loss: 1.716609886237275\n",
      "timestep: 676, loss: 1.6638699063421036\n",
      "timestep: 677, loss: 1.6165116302065392\n",
      "timestep: 678, loss: 1.5725007167029121\n",
      "timestep: 679, loss: 1.5302029748801191\n",
      "timestep: 680, loss: 1.506561281425334\n",
      "timestep: 681, loss: 1.500491449539638\n",
      "timestep: 682, loss: 1.499672447378442\n",
      "timestep: 683, loss: 1.5162143015380303\n",
      "timestep: 684, loss: 1.5489313483968359\n",
      "timestep: 685, loss: 1.5843309109651236\n",
      "timestep: 686, loss: 1.6337017662874374\n",
      "timestep: 687, loss: 1.693794891592299\n",
      "timestep: 688, loss: 1.744512482733941\n",
      "timestep: 689, loss: 1.7918491796742742\n",
      "timestep: 690, loss: 1.8314410759240192\n",
      "timestep: 691, loss: 1.8659355376994045\n",
      "timestep: 692, loss: 1.8890324690325444\n",
      "timestep: 693, loss: 1.8864074809471325\n",
      "timestep: 694, loss: 1.8693747654338864\n",
      "timestep: 695, loss: 1.833742258337147\n",
      "timestep: 696, loss: 1.786792228323828\n",
      "timestep: 697, loss: 1.7227923261981337\n",
      "timestep: 698, loss: 1.6640514389217862\n",
      "timestep: 699, loss: 1.6143569943045308\n",
      "timestep: 700, loss: 1.572297471553751\n",
      "timestep: 701, loss: 1.5472587279599581\n",
      "timestep: 702, loss: 1.5346336970461862\n",
      "timestep: 703, loss: 1.5340780467236732\n",
      "timestep: 704, loss: 1.5448534604689734\n",
      "timestep: 705, loss: 1.5677465350796573\n",
      "timestep: 706, loss: 1.5934569111505128\n",
      "timestep: 707, loss: 1.622400364276906\n",
      "timestep: 708, loss: 1.6495106179400323\n",
      "timestep: 709, loss: 1.6749852076293592\n",
      "timestep: 710, loss: 1.6963515928278377\n",
      "timestep: 711, loss: 1.7184263458625453\n",
      "timestep: 712, loss: 1.726854680021588\n",
      "timestep: 713, loss: 1.7235989058082175\n",
      "timestep: 714, loss: 1.7191129824859999\n",
      "timestep: 715, loss: 1.708081915291556\n",
      "timestep: 716, loss: 1.6892183542888461\n",
      "timestep: 717, loss: 1.6683805696627119\n",
      "timestep: 718, loss: 1.638776576121598\n",
      "timestep: 719, loss: 1.5902618725375874\n",
      "timestep: 720, loss: 1.5276914468145057\n",
      "timestep: 721, loss: 1.4655490943479317\n",
      "timestep: 722, loss: 1.382962548921829\n",
      "timestep: 723, loss: 1.2699414573558168\n",
      "timestep: 724, loss: 1.1245916643037885\n",
      "timestep: 725, loss: 0.9686600239348512\n",
      "timestep: 726, loss: 0.8320374547529351\n",
      "timestep: 727, loss: 0.7308440034356355\n",
      "timestep: 728, loss: 0.6616990646179716\n",
      "timestep: 729, loss: 0.6227477986244168\n",
      "timestep: 730, loss: 0.6060656585959046\n",
      "timestep: 731, loss: 0.608957156920476\n",
      "timestep: 732, loss: 0.6263935716627971\n",
      "timestep: 733, loss: 0.6627196215488175\n",
      "timestep: 734, loss: 0.7172397000398214\n",
      "timestep: 735, loss: 0.7881143098760245\n",
      "timestep: 736, loss: 0.8746645295013274\n",
      "timestep: 737, loss: 0.9840949354965409\n",
      "timestep: 738, loss: 1.1091918666165357\n",
      "timestep: 739, loss: 1.2581100112719557\n",
      "timestep: 740, loss: 1.4253517755829086\n",
      "timestep: 741, loss: 1.6092727167980498\n",
      "timestep: 742, loss: 1.8045511819320152\n",
      "timestep: 743, loss: 1.9898603034535334\n",
      "timestep: 744, loss: 2.1467031209499363\n",
      "timestep: 745, loss: 2.263697611535223\n",
      "timestep: 746, loss: 2.3000025391743883\n",
      "timestep: 747, loss: 2.2532314135775127\n",
      "timestep: 748, loss: 2.1359364488425694\n",
      "timestep: 749, loss: 1.9653820341796182\n",
      "timestep: 750, loss: 1.7607645049571985\n",
      "timestep: 751, loss: 1.5587543849579049\n",
      "timestep: 752, loss: 1.3698578909706258\n",
      "timestep: 753, loss: 1.2153690166865454\n",
      "timestep: 754, loss: 1.093381443199694\n",
      "timestep: 755, loss: 1.0123749711377314\n",
      "timestep: 756, loss: 0.9582316178959183\n",
      "timestep: 757, loss: 0.9386678871376872\n",
      "timestep: 758, loss: 0.9369027332489168\n",
      "timestep: 759, loss: 0.9678951996643176\n",
      "timestep: 760, loss: 1.019395728929779\n",
      "timestep: 761, loss: 1.0914815162746108\n",
      "timestep: 762, loss: 1.1779460985958659\n",
      "timestep: 763, loss: 1.288964924360746\n",
      "timestep: 764, loss: 1.412555764712063\n",
      "timestep: 765, loss: 1.5490906672207208\n",
      "timestep: 766, loss: 1.689127872091082\n",
      "timestep: 767, loss: 1.8252579371894653\n",
      "timestep: 768, loss: 1.9413877789172034\n",
      "timestep: 769, loss: 2.0414312197967717\n",
      "timestep: 770, loss: 2.0898946047224456\n",
      "timestep: 771, loss: 2.076990573874858\n",
      "timestep: 772, loss: 2.007871228193053\n",
      "timestep: 773, loss: 1.8938115448724957\n",
      "timestep: 774, loss: 1.7674791651718527\n",
      "timestep: 775, loss: 1.6130866171700087\n",
      "timestep: 776, loss: 1.4393874380228329\n",
      "timestep: 777, loss: 1.2849639605114187\n",
      "timestep: 778, loss: 1.1421864776919826\n",
      "timestep: 779, loss: 1.0311204112614052\n",
      "timestep: 780, loss: 0.9462160401661778\n",
      "timestep: 781, loss: 0.8920484205119054\n",
      "timestep: 782, loss: 0.8551439294468138\n",
      "timestep: 783, loss: 0.850081374505238\n",
      "timestep: 784, loss: 0.8624407463328911\n",
      "timestep: 785, loss: 0.8951968306571044\n",
      "timestep: 786, loss: 0.9454447123091471\n",
      "timestep: 787, loss: 1.0225915494865532\n",
      "timestep: 788, loss: 1.1173757855736715\n",
      "timestep: 789, loss: 1.2352856996817672\n",
      "timestep: 790, loss: 1.3757672431622738\n",
      "timestep: 791, loss: 1.5345556760718422\n",
      "timestep: 792, loss: 1.7086617736494092\n",
      "timestep: 793, loss: 1.894052031325593\n",
      "timestep: 794, loss: 2.0752577178548735\n",
      "timestep: 795, loss: 2.2279005223742114\n",
      "timestep: 796, loss: 2.3233254722743015\n",
      "timestep: 797, loss: 2.339517615294629\n",
      "timestep: 798, loss: 2.26604809696664\n",
      "timestep: 799, loss: 2.1416010082675703\n",
      "timestep: 800, loss: 1.9205199554999124\n",
      "timestep: 801, loss: 1.65536331394239\n",
      "timestep: 802, loss: 1.388246000172252\n",
      "timestep: 803, loss: 1.150905863192351\n",
      "timestep: 804, loss: 0.9593487350420177\n",
      "timestep: 805, loss: 0.8125043902180645\n",
      "timestep: 806, loss: 0.7080809300344982\n",
      "timestep: 807, loss: 0.6445240096122314\n",
      "timestep: 808, loss: 0.6115252511186898\n",
      "timestep: 809, loss: 0.6044088005184536\n",
      "timestep: 810, loss: 0.6178212502131143\n",
      "timestep: 811, loss: 0.6518318730700268\n",
      "timestep: 812, loss: 0.7068432653599243\n",
      "timestep: 813, loss: 0.7903056467623472\n",
      "timestep: 814, loss: 0.9040488033638642\n",
      "timestep: 815, loss: 1.050125978643196\n",
      "timestep: 816, loss: 1.2330267846014447\n",
      "timestep: 817, loss: 1.4610204537696112\n",
      "timestep: 818, loss: 1.7310965550218533\n",
      "timestep: 819, loss: 2.0475870960682596\n",
      "timestep: 820, loss: 2.3892064812119815\n",
      "timestep: 821, loss: 2.7206585680140485\n",
      "timestep: 822, loss: 2.9937421409334206\n",
      "timestep: 823, loss: 3.1370348072167684\n",
      "timestep: 824, loss: 3.072854624769002\n",
      "timestep: 825, loss: 2.7803620926480614\n",
      "timestep: 826, loss: 2.313034139664933\n",
      "timestep: 827, loss: 1.780176086541263\n",
      "timestep: 828, loss: 1.2924492808465913\n",
      "timestep: 829, loss: 0.9118092127681215\n",
      "timestep: 830, loss: 0.6433562214284954\n",
      "timestep: 831, loss: 0.47124323849056005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 832, loss: 0.36480915903728134\n",
      "timestep: 833, loss: 0.32222634103448455\n",
      "timestep: 834, loss: 0.3151563327395382\n",
      "timestep: 835, loss: 0.3215556555258851\n",
      "timestep: 836, loss: 0.33430220921997905\n",
      "timestep: 837, loss: 0.3582420912686205\n",
      "timestep: 838, loss: 0.3940167242817916\n",
      "timestep: 839, loss: 0.44803697195154074\n",
      "timestep: 840, loss: 0.5203029732865256\n",
      "timestep: 841, loss: 0.6152502872840445\n",
      "timestep: 842, loss: 0.734514673274861\n",
      "timestep: 843, loss: 0.8840016435007261\n",
      "timestep: 844, loss: 1.0766011363745358\n",
      "timestep: 845, loss: 1.3243882540301355\n",
      "timestep: 846, loss: 1.6395765122696473\n",
      "timestep: 847, loss: 2.0266477639424787\n",
      "timestep: 848, loss: 2.502990440722496\n",
      "timestep: 849, loss: 3.0779209214471575\n",
      "timestep: 850, loss: 3.6535239975143585\n",
      "timestep: 851, loss: 4.108087522364664\n",
      "timestep: 852, loss: 4.232371917528979\n",
      "timestep: 853, loss: 3.8666878049377615\n",
      "timestep: 854, loss: 3.018257904370089\n",
      "timestep: 855, loss: 2.006587918319728\n",
      "timestep: 856, loss: 1.1593247224994137\n",
      "timestep: 857, loss: 0.6126438947303824\n",
      "timestep: 858, loss: 0.3209031218355561\n",
      "timestep: 859, loss: 0.18449076992283298\n",
      "timestep: 860, loss: 0.13092524724264906\n",
      "timestep: 861, loss: 0.11060757410901158\n",
      "timestep: 862, loss: 0.1046844981418404\n",
      "timestep: 863, loss: 0.1009328626031459\n",
      "timestep: 864, loss: 0.09505142747570633\n",
      "timestep: 865, loss: 0.09443909281248866\n",
      "timestep: 866, loss: 0.09563212958026918\n",
      "timestep: 867, loss: 0.09643715287811964\n",
      "timestep: 868, loss: 0.09804336585364683\n",
      "timestep: 869, loss: 0.09842398493092448\n",
      "timestep: 870, loss: 0.09788106023997324\n",
      "timestep: 871, loss: 0.09682814877106397\n",
      "timestep: 872, loss: 0.09490295465032682\n",
      "timestep: 873, loss: 0.09181703378252005\n",
      "timestep: 874, loss: 0.08708132240178362\n",
      "timestep: 875, loss: 0.08085140641934949\n",
      "timestep: 876, loss: 0.07457539681710464\n",
      "timestep: 877, loss: 0.06873008764398271\n",
      "timestep: 878, loss: 0.06364420445959981\n",
      "timestep: 879, loss: 0.06102464043551863\n",
      "timestep: 880, loss: 0.06002836651612284\n",
      "timestep: 881, loss: 0.059702126688190416\n",
      "timestep: 882, loss: 0.0594783464511705\n",
      "timestep: 883, loss: 0.05916754313995692\n",
      "timestep: 884, loss: 0.0588179048361259\n",
      "timestep: 885, loss: 0.05849730918221041\n",
      "timestep: 886, loss: 0.058182199332255324\n",
      "timestep: 887, loss: 0.057857144118931364\n",
      "timestep: 888, loss: 0.05743838562047465\n",
      "timestep: 889, loss: 0.05705371671652224\n",
      "timestep: 890, loss: 0.05659815682123484\n",
      "timestep: 891, loss: 0.056260460537300855\n",
      "timestep: 892, loss: 0.05587948961225086\n",
      "timestep: 893, loss: 0.055531062421510756\n",
      "timestep: 894, loss: 0.05504144356493468\n",
      "timestep: 895, loss: 0.054681607565550405\n",
      "timestep: 896, loss: 0.054372605282047286\n",
      "timestep: 897, loss: 0.054018959157046235\n",
      "timestep: 898, loss: 0.054069983165534394\n",
      "timestep: 899, loss: 0.05347916436507096\n",
      "timestep: 900, loss: 0.0531510014884836\n",
      "timestep: 901, loss: 0.052903693304628234\n",
      "timestep: 902, loss: 0.05264609477746187\n",
      "timestep: 903, loss: 0.05263432903814735\n",
      "timestep: 904, loss: 0.051867444521605716\n",
      "timestep: 905, loss: 0.051068781647292776\n",
      "timestep: 906, loss: 0.050635292938419334\n",
      "timestep: 907, loss: 0.050230644050984916\n",
      "timestep: 908, loss: 0.050079724054030716\n",
      "timestep: 909, loss: 0.04993760686343302\n",
      "timestep: 910, loss: 0.04975141437990326\n",
      "timestep: 911, loss: 0.048721776260989544\n",
      "timestep: 912, loss: 0.04865607051300472\n",
      "timestep: 913, loss: 0.04803366260604762\n",
      "timestep: 914, loss: 0.0479159880051896\n",
      "timestep: 915, loss: 0.04742269389619847\n",
      "timestep: 916, loss: 0.04730424608927473\n",
      "timestep: 917, loss: 0.04711960624246719\n",
      "timestep: 918, loss: 0.04686830322793571\n",
      "timestep: 919, loss: 0.04693774445279886\n",
      "timestep: 920, loss: 0.04650082222895553\n",
      "timestep: 921, loss: 0.04652175247126195\n",
      "timestep: 922, loss: 0.04632329133378783\n",
      "timestep: 923, loss: 0.046277670724683896\n",
      "timestep: 924, loss: 0.04639363757440464\n",
      "timestep: 925, loss: 0.046024883691842405\n",
      "timestep: 926, loss: 0.045728147456895685\n",
      "timestep: 927, loss: 0.04582289967136126\n",
      "timestep: 928, loss: 0.04593903707986932\n",
      "timestep: 929, loss: 0.04559820394162761\n",
      "timestep: 930, loss: 0.045461531714487295\n",
      "timestep: 931, loss: 0.04573233018078849\n",
      "timestep: 932, loss: 0.044775444432537954\n",
      "timestep: 933, loss: 0.04469994551082576\n",
      "timestep: 934, loss: 0.04538969207544914\n",
      "timestep: 935, loss: 0.0451269356136873\n",
      "timestep: 936, loss: 0.04530502815188165\n",
      "timestep: 937, loss: 0.04561386433912237\n",
      "timestep: 938, loss: 0.04561681650221558\n",
      "timestep: 939, loss: 0.0456337465841463\n",
      "timestep: 940, loss: 0.04594118363276112\n",
      "timestep: 941, loss: 0.04615494263241542\n",
      "timestep: 942, loss: 0.0463343524473397\n",
      "timestep: 943, loss: 0.04645077213056641\n",
      "timestep: 944, loss: 0.0460849387210843\n",
      "timestep: 945, loss: 0.04631334868734493\n",
      "timestep: 946, loss: 0.047128463890006064\n",
      "timestep: 947, loss: 0.048422800877402895\n",
      "timestep: 948, loss: 0.04899811072272615\n",
      "timestep: 949, loss: 0.04811319963276695\n",
      "timestep: 950, loss: 0.04736044035718592\n",
      "timestep: 951, loss: 0.9127792953078551\n",
      "timestep: 952, loss: 0.8683447222373004\n",
      "timestep: 953, loss: 0.2800057396885325\n",
      "timestep: 954, loss: 0.1290638613681437\n",
      "timestep: 955, loss: 0.12700500680456414\n",
      "timestep: 956, loss: 0.0747725343554517\n",
      "timestep: 957, loss: 0.08537879863738929\n",
      "timestep: 958, loss: 0.081386842241785\n",
      "timestep: 959, loss: 0.07321634176662377\n",
      "timestep: 960, loss: 0.07631488739738503\n",
      "timestep: 961, loss: 0.07682583266045814\n",
      "timestep: 962, loss: 0.07573323733356298\n",
      "timestep: 963, loss: 0.07852429764933878\n",
      "timestep: 964, loss: 0.07751585710702817\n",
      "timestep: 965, loss: 0.07549387280852583\n",
      "timestep: 966, loss: 0.07701456491428672\n",
      "timestep: 967, loss: 0.08040286682046617\n",
      "timestep: 968, loss: 0.0806558749001288\n",
      "timestep: 969, loss: 0.08084115730434453\n",
      "timestep: 970, loss: 0.08447345733161273\n",
      "timestep: 971, loss: 0.08650688254825013\n",
      "timestep: 972, loss: 0.08578164620417113\n",
      "timestep: 973, loss: 0.08407564623539275\n",
      "timestep: 974, loss: 0.08542380754080618\n",
      "timestep: 975, loss: 0.09079442145106803\n",
      "timestep: 976, loss: 0.09237468488210315\n",
      "timestep: 977, loss: 0.09244592898720616\n",
      "timestep: 978, loss: 0.09433734420812807\n",
      "timestep: 979, loss: 0.0975956625199068\n",
      "timestep: 980, loss: 0.10170303083327258\n",
      "timestep: 981, loss: 0.10428124643545834\n",
      "timestep: 982, loss: 0.10778381060547808\n",
      "timestep: 983, loss: 0.11219038983342565\n",
      "timestep: 984, loss: 0.11535165131493086\n",
      "timestep: 985, loss: 0.11901469134586985\n",
      "timestep: 986, loss: 0.12323489364234211\n",
      "timestep: 987, loss: 0.12734189397859982\n",
      "timestep: 988, loss: 0.1327437507383101\n",
      "timestep: 989, loss: 0.13819241458644127\n",
      "timestep: 990, loss: 0.14511908180974975\n",
      "timestep: 991, loss: 0.14950029768644832\n",
      "timestep: 992, loss: 0.15292013673765245\n",
      "timestep: 993, loss: 0.15599524913287824\n",
      "timestep: 994, loss: 0.16084717961219536\n",
      "timestep: 995, loss: 0.16366843167395612\n",
      "timestep: 996, loss: 0.16775182693446075\n",
      "timestep: 997, loss: 0.174572365532723\n",
      "timestep: 998, loss: 0.17607872501591892\n",
      "timestep: 999, loss: 0.178573517167088\n",
      "timestep: 1000, loss: 0.17597607847455182\n",
      "timestep: 1001, loss: 0.17695922279345772\n",
      "timestep: 1002, loss: 0.1806747795078076\n",
      "timestep: 1003, loss: 0.18007543415346616\n",
      "timestep: 1004, loss: 0.1791495936957845\n",
      "timestep: 1005, loss: 0.18157306206061194\n",
      "timestep: 1006, loss: 0.1817021906470223\n",
      "timestep: 1007, loss: 0.17934969942188952\n",
      "timestep: 1008, loss: 0.17896683802220661\n",
      "timestep: 1009, loss: 0.17522499013552734\n",
      "timestep: 1010, loss: 0.1719272724579173\n",
      "timestep: 1011, loss: 0.1695530236440818\n",
      "timestep: 1012, loss: 0.1668641427110438\n",
      "timestep: 1013, loss: 0.17018719471529586\n",
      "timestep: 1014, loss: 0.16924710367546514\n",
      "timestep: 1015, loss: 0.17193417331285032\n",
      "timestep: 1016, loss: 0.17572167505998781\n",
      "timestep: 1017, loss: 0.17197693370804373\n",
      "timestep: 1018, loss: 0.16481887168146217\n",
      "timestep: 1019, loss: 0.16053037181349925\n",
      "timestep: 1020, loss: 0.15735785373742855\n",
      "timestep: 1021, loss: 0.15231164256676955\n",
      "timestep: 1022, loss: 0.14697779689565652\n",
      "timestep: 1023, loss: 0.14068635151339723\n",
      "timestep: 1024, loss: 0.13343342672733016\n",
      "timestep: 1025, loss: 0.12743524120362848\n",
      "timestep: 1026, loss: 0.12142017462387357\n",
      "timestep: 1027, loss: 0.11472236552038684\n",
      "timestep: 1028, loss: 0.10922138683370931\n",
      "timestep: 1029, loss: 0.10488807452524686\n",
      "timestep: 1030, loss: 0.0988420698543297\n",
      "timestep: 1031, loss: 0.09322587792601258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 1032, loss: 0.08824367008154756\n",
      "timestep: 1033, loss: 0.08438263273375837\n",
      "timestep: 1034, loss: 0.08073169615737387\n",
      "timestep: 1035, loss: 0.07726833843250724\n",
      "timestep: 1036, loss: 0.07460368691090058\n",
      "timestep: 1037, loss: 0.07177422940921398\n",
      "timestep: 1038, loss: 0.06835789279544588\n",
      "timestep: 1039, loss: 0.06593128977102115\n",
      "timestep: 1040, loss: 0.06358677722927246\n",
      "timestep: 1041, loss: 0.061478425539646046\n",
      "timestep: 1042, loss: 0.05957468013273545\n",
      "timestep: 1043, loss: 0.05782845844139053\n",
      "timestep: 1044, loss: 0.05622406731634539\n",
      "timestep: 1045, loss: 0.05478390664509725\n",
      "timestep: 1046, loss: 0.05310945034465009\n",
      "timestep: 1047, loss: 0.05178454265900283\n",
      "timestep: 1048, loss: 0.05029575006721649\n",
      "timestep: 1049, loss: 0.049613344575967555\n",
      "timestep: 1050, loss: 0.04752193622154841\n",
      "timestep: 1051, loss: 0.04928766985912004\n",
      "timestep: 1052, loss: 0.04877866084766578\n",
      "timestep: 1053, loss: 0.07177788920989084\n",
      "timestep: 1054, loss: 0.14173530148438518\n",
      "timestep: 1055, loss: 0.5404186684688381\n",
      "timestep: 1056, loss: 2.0367162492453597\n",
      "timestep: 1057, loss: 4.310250955920226\n",
      "timestep: 1058, loss: 1.441620340691623\n",
      "timestep: 1059, loss: 1.0444093344669283\n",
      "timestep: 1060, loss: 1.8661664133553935\n",
      "timestep: 1061, loss: 2.5737741800646674\n",
      "timestep: 1062, loss: 2.603945169113056\n",
      "timestep: 1063, loss: 1.793653132139963\n",
      "timestep: 1064, loss: 1.2226082529530884\n",
      "timestep: 1065, loss: 0.7226938148928691\n",
      "timestep: 1066, loss: 0.37873726510469813\n",
      "timestep: 1067, loss: 0.8010963885147977\n",
      "timestep: 1068, loss: 0.5052711035854344\n",
      "timestep: 1069, loss: 0.33395369749564385\n",
      "timestep: 1070, loss: 0.19638451530170364\n",
      "timestep: 1071, loss: 0.1050216881140424\n",
      "timestep: 1072, loss: 0.22597607972909345\n",
      "timestep: 1073, loss: 0.18409842320666395\n",
      "timestep: 1074, loss: 0.133809636186375\n",
      "timestep: 1075, loss: 0.07799686155568498\n",
      "timestep: 1076, loss: 0.042943893329070275\n",
      "timestep: 1077, loss: 0.10373297839256117\n",
      "timestep: 1078, loss: 0.12190375968251561\n",
      "timestep: 1079, loss: 0.10236357842403089\n",
      "timestep: 1080, loss: 0.07726184523678666\n",
      "timestep: 1081, loss: 0.043600256483037364\n",
      "timestep: 1082, loss: 0.03377374539957356\n",
      "timestep: 1083, loss: 0.0591884522107504\n",
      "timestep: 1084, loss: 0.05676838572619129\n",
      "timestep: 1085, loss: 0.07625295197394992\n",
      "timestep: 1086, loss: 0.07002340194150927\n",
      "timestep: 1087, loss: 0.08034322941186528\n",
      "timestep: 1088, loss: 0.08063058433904234\n",
      "timestep: 1089, loss: 0.0840227449814085\n",
      "timestep: 1090, loss: 0.08527143495689865\n",
      "timestep: 1091, loss: 0.09105807316111214\n",
      "timestep: 1092, loss: 0.08552823782456477\n",
      "timestep: 1093, loss: 0.09430705691630106\n",
      "timestep: 1094, loss: 0.08085903569656061\n",
      "timestep: 1095, loss: 0.09148311597056386\n",
      "timestep: 1096, loss: 0.0716773624778447\n",
      "timestep: 1097, loss: 0.08503984052764019\n",
      "timestep: 1098, loss: 0.06917056290864171\n",
      "timestep: 1099, loss: 0.078211235991201\n",
      "timestep: 1100, loss: 0.06033706878211886\n",
      "timestep: 1101, loss: 0.07652558610033215\n",
      "timestep: 1102, loss: 0.05754329637853299\n",
      "timestep: 1103, loss: 0.07062732299951374\n",
      "timestep: 1104, loss: 0.046268097168800267\n",
      "timestep: 1105, loss: 0.0671259118376795\n",
      "timestep: 1106, loss: 0.049116142471606755\n",
      "timestep: 1107, loss: 0.06859567672542363\n",
      "timestep: 1108, loss: 0.03726882309730674\n",
      "timestep: 1109, loss: 0.07891751002457398\n",
      "timestep: 1110, loss: 0.03053391234260815\n",
      "timestep: 1111, loss: 0.07143711235958977\n",
      "timestep: 1112, loss: 0.0415776108863187\n",
      "timestep: 1113, loss: 0.1247386328189236\n",
      "timestep: 1114, loss: 0.06174036214526371\n",
      "timestep: 1115, loss: 0.08778549012358512\n",
      "timestep: 1116, loss: 0.05413011052028072\n",
      "timestep: 1117, loss: 0.033970875269133206\n",
      "timestep: 1118, loss: 0.04679408764663661\n",
      "timestep: 1119, loss: 0.03640299363056712\n",
      "timestep: 1120, loss: 0.033171073688158824\n",
      "timestep: 1121, loss: 0.03511462740833877\n",
      "timestep: 1122, loss: 0.03103520526111572\n",
      "timestep: 1123, loss: 0.03176093925959184\n",
      "timestep: 1124, loss: 0.028914188362819636\n",
      "timestep: 1125, loss: 0.0293868586460852\n",
      "timestep: 1126, loss: 0.026896681104713997\n",
      "timestep: 1127, loss: 0.027692790703043677\n",
      "timestep: 1128, loss: 0.02460021961940676\n",
      "timestep: 1129, loss: 0.02768475455746009\n",
      "timestep: 1130, loss: 0.02282479596219572\n",
      "timestep: 1131, loss: 0.039054407894327146\n",
      "timestep: 1132, loss: 0.09098368188218448\n",
      "timestep: 1133, loss: 0.3916155926855561\n",
      "timestep: 1134, loss: 1.0686937541420953\n",
      "timestep: 1135, loss: 2.214599377770509\n",
      "timestep: 1136, loss: 0.72374212211253\n",
      "timestep: 1137, loss: 0.052555811472610944\n",
      "timestep: 1138, loss: 0.13858607241573564\n",
      "timestep: 1139, loss: 0.08344461703847467\n",
      "timestep: 1140, loss: 0.04190220947657892\n",
      "timestep: 1141, loss: 0.05111779102414085\n",
      "timestep: 1142, loss: 0.047992360507148744\n",
      "timestep: 1143, loss: 0.039110904878751636\n",
      "timestep: 1144, loss: 0.03787158934513225\n",
      "timestep: 1145, loss: 0.03369384383946599\n",
      "timestep: 1146, loss: 0.031372612293181716\n",
      "timestep: 1147, loss: 0.027459272500296627\n",
      "timestep: 1148, loss: 0.02418323120040465\n",
      "timestep: 1149, loss: 0.0291370769317808\n",
      "timestep: 1150, loss: 0.04044693441786831\n",
      "timestep: 1151, loss: 0.11158663533518554\n",
      "timestep: 1152, loss: 0.2428496245171926\n",
      "timestep: 1153, loss: 0.7239291760798179\n",
      "timestep: 1154, loss: 1.0982840281702513\n",
      "timestep: 1155, loss: 0.8469584466859937\n",
      "timestep: 1156, loss: 0.20607620258724005\n",
      "timestep: 1157, loss: 0.0291883125475832\n",
      "timestep: 1158, loss: 0.06917625777946934\n",
      "timestep: 1159, loss: 0.038441847913719344\n",
      "timestep: 1160, loss: 0.03743022917773355\n",
      "timestep: 1161, loss: 0.030183852292398308\n",
      "timestep: 1162, loss: 0.030436661256163497\n",
      "timestep: 1163, loss: 0.027427830134297847\n",
      "timestep: 1164, loss: 0.02676357222206197\n",
      "timestep: 1165, loss: 0.023491698999111986\n",
      "timestep: 1166, loss: 0.02279174034481748\n",
      "timestep: 1167, loss: 0.028660141008939058\n",
      "timestep: 1168, loss: 0.025592926505873723\n",
      "timestep: 1169, loss: 0.03882034196696011\n",
      "timestep: 1170, loss: 0.08515024068156823\n",
      "timestep: 1171, loss: 0.2301244861891757\n",
      "timestep: 1172, loss: 1.1297952665654103\n",
      "timestep: 1173, loss: 3.0651332130978712\n",
      "timestep: 1174, loss: 1.210478714433751\n",
      "timestep: 1175, loss: 0.5823760394090051\n",
      "timestep: 1176, loss: 0.14317602543082905\n",
      "timestep: 1177, loss: 0.03798923581927513\n",
      "timestep: 1178, loss: 0.0830492593004626\n",
      "timestep: 1179, loss: 0.059410578548126966\n",
      "timestep: 1180, loss: 0.03844381126476471\n",
      "timestep: 1181, loss: 0.0441646487035864\n",
      "timestep: 1182, loss: 0.03862003338850826\n",
      "timestep: 1183, loss: 0.033805404556101926\n",
      "timestep: 1184, loss: 0.033683973512519616\n",
      "timestep: 1185, loss: 0.031219351085054026\n",
      "timestep: 1186, loss: 0.030160681492628447\n",
      "timestep: 1187, loss: 0.029057185788983666\n",
      "timestep: 1188, loss: 0.027504212960514408\n",
      "timestep: 1189, loss: 0.027512152983011618\n",
      "timestep: 1190, loss: 0.025990987808615987\n",
      "timestep: 1191, loss: 0.02627535524685084\n",
      "timestep: 1192, loss: 0.023288756388475204\n",
      "timestep: 1193, loss: 0.02516133208050686\n",
      "timestep: 1194, loss: 0.03512307761136191\n",
      "timestep: 1195, loss: 0.08354115636950342\n",
      "timestep: 1196, loss: 0.22983287676872993\n",
      "timestep: 1197, loss: 0.9763867234488455\n",
      "timestep: 1198, loss: 2.172238350328473\n",
      "timestep: 1199, loss: 1.2143872816871666\n",
      "timestep: 1200, loss: 0.3178153469989492\n",
      "timestep: 1201, loss: 0.06526889533101571\n",
      "timestep: 1202, loss: 0.04425884365170143\n",
      "timestep: 1203, loss: 0.08223525351393779\n",
      "timestep: 1204, loss: 0.04050178179378395\n",
      "timestep: 1205, loss: 0.035466467770124625\n",
      "timestep: 1206, loss: 0.04001456916475505\n",
      "timestep: 1207, loss: 0.02793867373772606\n",
      "timestep: 1208, loss: 0.028195235725803448\n",
      "timestep: 1209, loss: 0.04869437496671824\n",
      "timestep: 1210, loss: 0.026894016665619445\n",
      "timestep: 1211, loss: 0.09837453129551504\n",
      "timestep: 1212, loss: 0.029777868601859603\n",
      "timestep: 1213, loss: 0.10046692510643783\n",
      "timestep: 1214, loss: 0.047289710002538315\n",
      "timestep: 1215, loss: 0.18143523422035898\n",
      "timestep: 1216, loss: 0.1659074926301649\n",
      "timestep: 1217, loss: 0.5457218828095786\n",
      "timestep: 1218, loss: 0.5334206696528484\n",
      "timestep: 1219, loss: 0.6791317758633414\n",
      "timestep: 1220, loss: 0.14675782958380548\n",
      "timestep: 1221, loss: 0.07681303477067493\n",
      "timestep: 1222, loss: 0.034856111132983786\n",
      "timestep: 1223, loss: 0.025128250578734922\n",
      "timestep: 1224, loss: 0.04775566502819259\n",
      "timestep: 1225, loss: 0.02408834941126049\n",
      "timestep: 1226, loss: 0.0494880174595429\n",
      "timestep: 1227, loss: 0.02492467848041764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 1228, loss: 0.08045913075096986\n",
      "timestep: 1229, loss: 0.07036071679643062\n",
      "timestep: 1230, loss: 0.3388963001030165\n",
      "timestep: 1231, loss: 0.6676612740500084\n",
      "timestep: 1232, loss: 1.8284863038044377\n",
      "timestep: 1233, loss: 0.8686945426857423\n",
      "timestep: 1234, loss: 0.09867739391485385\n",
      "timestep: 1235, loss: 0.15403134881341338\n",
      "timestep: 1236, loss: 0.034305595217246763\n",
      "timestep: 1237, loss: 0.05341003430140721\n",
      "timestep: 1238, loss: 0.05890100349878209\n",
      "timestep: 1239, loss: 0.031193887130542542\n",
      "timestep: 1240, loss: 0.04495992633355244\n",
      "timestep: 1241, loss: 0.03592628769796157\n",
      "timestep: 1242, loss: 0.039170292021012215\n",
      "timestep: 1243, loss: 0.03700236405320625\n",
      "timestep: 1244, loss: 0.03642616165430158\n",
      "timestep: 1245, loss: 0.03442866089585506\n",
      "timestep: 1246, loss: 0.03723964906738221\n",
      "timestep: 1247, loss: 0.029456975386890127\n",
      "timestep: 1248, loss: 0.04192619133844267\n",
      "timestep: 1249, loss: 0.026808409061867454\n",
      "timestep: 1250, loss: 0.08166200060238361\n",
      "timestep: 1251, loss: 0.10671280704274438\n",
      "timestep: 1252, loss: 0.6074749158296071\n",
      "timestep: 1253, loss: 1.5452333150808213\n",
      "timestep: 1254, loss: 2.6783047785314613\n",
      "timestep: 1255, loss: 0.273981944634708\n",
      "timestep: 1256, loss: 0.041330380165218034\n",
      "timestep: 1257, loss: 0.07964562820719406\n",
      "timestep: 1258, loss: 0.08600658702544998\n",
      "timestep: 1259, loss: 0.06752375496185638\n",
      "timestep: 1260, loss: 0.09814529129311188\n",
      "timestep: 1261, loss: 0.10070610205654035\n",
      "timestep: 1262, loss: 0.08778779537079463\n",
      "timestep: 1263, loss: 0.0903343857442008\n",
      "timestep: 1264, loss: 0.09433004942971843\n",
      "timestep: 1265, loss: 0.08550288956236729\n",
      "timestep: 1266, loss: 0.09418722578612658\n",
      "timestep: 1267, loss: 0.08301963005960226\n",
      "timestep: 1268, loss: 0.09338075733342684\n",
      "timestep: 1269, loss: 0.0755393016644163\n",
      "timestep: 1270, loss: 0.0945513029669618\n",
      "timestep: 1271, loss: 0.06663785366991061\n",
      "timestep: 1272, loss: 0.10311693819068074\n",
      "timestep: 1273, loss: 0.056248422488348465\n",
      "timestep: 1274, loss: 0.13247734926153523\n",
      "timestep: 1275, loss: 0.05437363973844696\n",
      "timestep: 1276, loss: 0.2524399932116766\n",
      "timestep: 1277, loss: 0.20467612738477134\n",
      "timestep: 1278, loss: 0.9334948624600443\n",
      "timestep: 1279, loss: 0.8327510955441351\n",
      "timestep: 1280, loss: 1.055192059203405\n",
      "timestep: 1281, loss: 0.04593099046581868\n",
      "timestep: 1282, loss: 0.10071109440203557\n",
      "timestep: 1283, loss: 0.17949928246642752\n",
      "timestep: 1284, loss: 0.07401850062123587\n",
      "timestep: 1285, loss: 0.12534911920268466\n",
      "timestep: 1286, loss: 0.10617565028212889\n",
      "timestep: 1287, loss: 0.11016686830749527\n",
      "timestep: 1288, loss: 0.1059387652100682\n",
      "timestep: 1289, loss: 0.10500368953519151\n",
      "timestep: 1290, loss: 0.10836391823004009\n",
      "timestep: 1291, loss: 0.09258898566688592\n",
      "timestep: 1292, loss: 0.1150235362139894\n",
      "timestep: 1293, loss: 0.08029534220562645\n",
      "timestep: 1294, loss: 0.1378047575377286\n",
      "timestep: 1295, loss: 0.06331242466922556\n",
      "timestep: 1296, loss: 0.2153816435649625\n",
      "timestep: 1297, loss: 0.1339429466040511\n",
      "timestep: 1298, loss: 0.7144283979637559\n",
      "timestep: 1299, loss: 0.6767076731465083\n",
      "timestep: 1300, loss: 1.3565831956590335\n",
      "timestep: 1301, loss: 0.10210401784649617\n",
      "timestep: 1302, loss: 0.20708972612104756\n",
      "timestep: 1303, loss: 0.31969677744488995\n",
      "timestep: 1304, loss: 0.1780804416881442\n",
      "timestep: 1305, loss: 0.24668454591611033\n",
      "timestep: 1306, loss: 0.25787604056377816\n",
      "timestep: 1307, loss: 0.2569189677463726\n",
      "timestep: 1308, loss: 0.26717423451474437\n",
      "timestep: 1309, loss: 0.2598999911865674\n",
      "timestep: 1310, loss: 0.2927724703427042\n",
      "timestep: 1311, loss: 0.27774441100862257\n",
      "timestep: 1312, loss: 0.26984288446335475\n",
      "timestep: 1313, loss: 0.30771075412698096\n",
      "timestep: 1314, loss: 0.2995143169365427\n",
      "timestep: 1315, loss: 0.33106059304092095\n",
      "timestep: 1316, loss: 0.36769967822645105\n",
      "timestep: 1317, loss: 0.3945145334621268\n",
      "timestep: 1318, loss: 0.3998022913021014\n",
      "timestep: 1319, loss: 0.4679199838982612\n",
      "timestep: 1320, loss: 0.4579044662766521\n",
      "timestep: 1321, loss: 0.4899292152596798\n",
      "timestep: 1322, loss: 0.612408896527925\n",
      "timestep: 1323, loss: 0.6786286537432479\n",
      "timestep: 1324, loss: 0.7719594205982223\n",
      "timestep: 1325, loss: 0.9510113697244171\n",
      "timestep: 1326, loss: 1.1828067738723385\n",
      "timestep: 1327, loss: 1.5702701981940392\n",
      "timestep: 1328, loss: 2.2438947986897806\n",
      "timestep: 1329, loss: 2.3475859816798033\n",
      "timestep: 1330, loss: 0.8650536585661347\n",
      "timestep: 1331, loss: 0.9337175408609512\n",
      "timestep: 1332, loss: 0.6719739750358467\n",
      "timestep: 1333, loss: 1.0637375415270744\n",
      "timestep: 1334, loss: 0.40531197304907496\n",
      "timestep: 1335, loss: 0.4813454835315411\n",
      "timestep: 1336, loss: 0.3570751765469988\n",
      "timestep: 1337, loss: 0.40297343898118715\n",
      "timestep: 1338, loss: 0.13783215047915637\n",
      "timestep: 1339, loss: 0.07290999559824908\n",
      "timestep: 1340, loss: 0.10588371885798765\n",
      "timestep: 1341, loss: 0.1635211790162203\n",
      "timestep: 1342, loss: 0.10644096732865367\n",
      "timestep: 1343, loss: 0.04270204683761476\n",
      "timestep: 1344, loss: 0.054746866558719874\n",
      "timestep: 1345, loss: 0.10664806449419122\n",
      "timestep: 1346, loss: 0.13411012274027825\n",
      "timestep: 1347, loss: 0.07479124596285822\n",
      "timestep: 1348, loss: 0.03571470337295452\n",
      "timestep: 1349, loss: 0.052314580140071905\n",
      "timestep: 1350, loss: 0.10683970609227184\n",
      "timestep: 1351, loss: 0.1502514305188319\n",
      "timestep: 1352, loss: 0.10625613593341682\n",
      "timestep: 1353, loss: 0.043699278800112\n",
      "timestep: 1354, loss: 0.038266490444315585\n",
      "timestep: 1355, loss: 0.07781258142564805\n",
      "timestep: 1356, loss: 0.14929077860180795\n",
      "timestep: 1357, loss: 0.18098114803400353\n",
      "timestep: 1358, loss: 0.1358615022038972\n",
      "timestep: 1359, loss: 0.06525620860628507\n",
      "timestep: 1360, loss: 0.03883576072531725\n",
      "timestep: 1361, loss: 0.057817423684116125\n",
      "timestep: 1362, loss: 0.11022962570093497\n",
      "timestep: 1363, loss: 0.17542916222402372\n",
      "timestep: 1364, loss: 0.20514678461486457\n",
      "timestep: 1365, loss: 0.1770214090045638\n",
      "timestep: 1366, loss: 0.11346125855874921\n",
      "timestep: 1367, loss: 0.06393414735993157\n",
      "timestep: 1368, loss: 0.04879196432895805\n",
      "timestep: 1369, loss: 0.06466696941693184\n",
      "timestep: 1370, loss: 0.10051927644703397\n",
      "timestep: 1371, loss: 0.14068704464545748\n",
      "timestep: 1372, loss: 0.17074422593617322\n",
      "timestep: 1373, loss: 0.18360454391216205\n",
      "timestep: 1374, loss: 0.18206477105049695\n",
      "timestep: 1375, loss: 0.17123403134913018\n",
      "timestep: 1376, loss: 0.15939225924239853\n",
      "timestep: 1377, loss: 0.15125038271916982\n",
      "timestep: 1378, loss: 0.14875173648615694\n",
      "timestep: 1379, loss: 0.15001456863239465\n",
      "timestep: 1380, loss: 0.15686328324913668\n",
      "timestep: 1381, loss: 0.16853052171836824\n",
      "timestep: 1382, loss: 0.182135658664673\n",
      "timestep: 1383, loss: 0.19541910921079453\n",
      "timestep: 1384, loss: 0.2061641429868969\n",
      "timestep: 1385, loss: 0.2184112870628371\n",
      "timestep: 1386, loss: 0.22955295683114996\n",
      "timestep: 1387, loss: 0.23932219074797348\n",
      "timestep: 1388, loss: 0.2513352898340137\n",
      "timestep: 1389, loss: 0.2662436361115782\n",
      "timestep: 1390, loss: 0.2855202473713565\n",
      "timestep: 1391, loss: 0.295775576255015\n",
      "timestep: 1392, loss: 0.2987222752876892\n",
      "timestep: 1393, loss: 0.30016765790446964\n",
      "timestep: 1394, loss: 0.3010784353613522\n",
      "timestep: 1395, loss: 0.3011084989346929\n",
      "timestep: 1396, loss: 0.29549925983798037\n",
      "timestep: 1397, loss: 0.2844999544782689\n",
      "timestep: 1398, loss: 0.271299209095017\n",
      "timestep: 1399, loss: 0.2541242513417647\n",
      "timestep: 1400, loss: 0.23652854601733075\n",
      "timestep: 1401, loss: 0.21838734390237968\n",
      "timestep: 1402, loss: 0.20483925451925825\n",
      "timestep: 1403, loss: 0.1922362430037275\n",
      "timestep: 1404, loss: 0.18365398550044157\n",
      "timestep: 1405, loss: 0.18003325877449006\n",
      "timestep: 1406, loss: 0.18076433846307868\n",
      "timestep: 1407, loss: 0.1837294833873859\n",
      "timestep: 1408, loss: 0.18824782963765177\n",
      "timestep: 1409, loss: 0.19252429844430027\n",
      "timestep: 1410, loss: 0.1977582097446686\n",
      "timestep: 1411, loss: 0.20328880993339474\n",
      "timestep: 1412, loss: 0.2051180162388457\n",
      "timestep: 1413, loss: 0.20569506664149537\n",
      "timestep: 1414, loss: 0.20862123233769206\n",
      "timestep: 1415, loss: 0.21206545657488868\n",
      "timestep: 1416, loss: 0.2188553305186736\n",
      "timestep: 1417, loss: 0.22671072051183092\n",
      "timestep: 1418, loss: 0.2336590459675457\n",
      "timestep: 1419, loss: 0.23847071980766843\n",
      "timestep: 1420, loss: 0.24838712977608726\n",
      "timestep: 1421, loss: 0.26266383598430487\n",
      "timestep: 1422, loss: 0.27242050373795623\n",
      "timestep: 1423, loss: 0.27740685488070466\n",
      "timestep: 1424, loss: 0.28549479981962106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 1425, loss: 0.29195673382257625\n",
      "timestep: 1426, loss: 0.306060925411325\n",
      "timestep: 1427, loss: 0.30370326178566653\n",
      "timestep: 1428, loss: 0.3214565456466682\n",
      "timestep: 1429, loss: 0.3269546495763302\n",
      "timestep: 1430, loss: 0.3284072370831929\n",
      "timestep: 1431, loss: 0.3433537882784786\n",
      "timestep: 1432, loss: 0.3479746527776222\n",
      "timestep: 1433, loss: 0.3431582973812821\n",
      "timestep: 1434, loss: 0.33223825290892584\n",
      "timestep: 1435, loss: 0.3152547929198617\n",
      "timestep: 1436, loss: 0.29028243223952543\n",
      "timestep: 1437, loss: 0.26725397837928605\n",
      "timestep: 1438, loss: 0.2347287641867234\n",
      "timestep: 1439, loss: 0.2045666876901861\n",
      "timestep: 1440, loss: 0.17930689628134067\n",
      "timestep: 1441, loss: 0.1573288873646628\n",
      "timestep: 1442, loss: 0.14309126373618636\n",
      "timestep: 1443, loss: 0.13475131993543082\n",
      "timestep: 1444, loss: 0.12562912455407854\n",
      "timestep: 1445, loss: 0.11693703232579145\n",
      "timestep: 1446, loss: 0.10952565130294438\n",
      "timestep: 1447, loss: 0.1023991415157485\n",
      "timestep: 1448, loss: 0.0981303388705634\n",
      "timestep: 1449, loss: 0.09468252300572161\n",
      "timestep: 1450, loss: 0.09095904885385081\n",
      "timestep: 1451, loss: 0.08858673096657256\n",
      "timestep: 1452, loss: 0.08737375501584378\n",
      "timestep: 1453, loss: 0.08780889561892126\n",
      "timestep: 1454, loss: 0.08906083360423483\n",
      "timestep: 1455, loss: 0.0899845960073721\n",
      "timestep: 1456, loss: 0.09355176334945324\n",
      "timestep: 1457, loss: 0.09498584267127831\n",
      "timestep: 1458, loss: 0.09441111640042488\n",
      "timestep: 1459, loss: 0.09576355048966961\n",
      "timestep: 1460, loss: 0.09703996560773649\n",
      "timestep: 1461, loss: 0.09984274615991494\n",
      "timestep: 1462, loss: 0.0997074210461479\n",
      "timestep: 1463, loss: 0.09795272992573346\n",
      "timestep: 1464, loss: 0.09983895572217412\n",
      "timestep: 1465, loss: 0.10294672811960663\n",
      "timestep: 1466, loss: 0.10081794724200796\n",
      "timestep: 1467, loss: 0.10969921308847258\n",
      "timestep: 1468, loss: 0.11339988196483114\n",
      "timestep: 1469, loss: 0.12333749334020906\n",
      "timestep: 1470, loss: 0.140471894080462\n",
      "timestep: 1471, loss: 0.15609034171239383\n",
      "timestep: 1472, loss: 0.14797731215496532\n",
      "timestep: 1473, loss: 0.15867893622478998\n",
      "timestep: 1474, loss: 0.17367386384116615\n",
      "timestep: 1475, loss: 0.1570131007816332\n",
      "timestep: 1476, loss: 0.18433973650570093\n",
      "timestep: 1477, loss: 0.16742331285578194\n",
      "timestep: 1478, loss: 0.2098293030622513\n",
      "timestep: 1479, loss: 0.1635833507024752\n",
      "timestep: 1480, loss: 0.23545190335539362\n",
      "timestep: 1481, loss: 0.16640076538907944\n",
      "timestep: 1482, loss: 0.2288805335457539\n",
      "timestep: 1483, loss: 0.2215542457901334\n",
      "timestep: 1484, loss: 0.24305533466804521\n",
      "timestep: 1485, loss: 0.24151309231220308\n",
      "timestep: 1486, loss: 0.2481743109945306\n",
      "timestep: 1487, loss: 0.2713050219060023\n",
      "timestep: 1488, loss: 0.28289356401331633\n",
      "timestep: 1489, loss: 0.32834477943772683\n",
      "timestep: 1490, loss: 0.3551738221347408\n",
      "timestep: 1491, loss: 0.3799242977340478\n",
      "timestep: 1492, loss: 0.41180524126053497\n",
      "timestep: 1493, loss: 0.47107875701079055\n",
      "timestep: 1494, loss: 0.5465993494006948\n",
      "timestep: 1495, loss: 0.6308313038286908\n",
      "timestep: 1496, loss: 0.7868940694228868\n",
      "timestep: 1497, loss: 1.0161694006236393\n",
      "timestep: 1498, loss: 1.4649244471345455\n",
      "timestep: 1499, loss: 2.272677902751473\n",
      "timestep: 1500, loss: 2.741519702380145\n",
      "timestep: 1501, loss: 1.3700868989440262\n",
      "timestep: 1502, loss: 1.3807032208619459\n",
      "timestep: 1503, loss: 0.6499289937161392\n",
      "timestep: 1504, loss: 0.9903305996571473\n",
      "timestep: 1505, loss: 0.4296335344299193\n",
      "timestep: 1506, loss: 0.3349979307467798\n",
      "timestep: 1507, loss: 0.16691345235852661\n",
      "timestep: 1508, loss: 0.1816822739354676\n",
      "timestep: 1509, loss: 0.06360607806878872\n",
      "timestep: 1510, loss: 0.029105867876647182\n",
      "timestep: 1511, loss: 0.04298088175581227\n",
      "timestep: 1512, loss: 0.0663115382183461\n",
      "timestep: 1513, loss: 0.04114693911555845\n",
      "timestep: 1514, loss: 0.02011070793603757\n",
      "timestep: 1515, loss: 0.022989742235192866\n",
      "timestep: 1516, loss: 0.042659576153936896\n",
      "timestep: 1517, loss: 0.043463148403033953\n",
      "timestep: 1518, loss: 0.026821966283861007\n",
      "timestep: 1519, loss: 0.016960632061641615\n",
      "timestep: 1520, loss: 0.024271016439865897\n",
      "timestep: 1521, loss: 0.04109263339747581\n",
      "timestep: 1522, loss: 0.04160990695652783\n",
      "timestep: 1523, loss: 0.0275868425577334\n",
      "timestep: 1524, loss: 0.01659301176034356\n",
      "timestep: 1525, loss: 0.018493130993046822\n",
      "timestep: 1526, loss: 0.034691927652308156\n",
      "timestep: 1527, loss: 0.046009145146824276\n",
      "timestep: 1528, loss: 0.03922160411481691\n",
      "timestep: 1529, loss: 0.025649141468644224\n",
      "timestep: 1530, loss: 0.016989523989783328\n",
      "timestep: 1531, loss: 0.018645006540784922\n",
      "timestep: 1532, loss: 0.03083282044299163\n",
      "timestep: 1533, loss: 0.04229711113711446\n",
      "timestep: 1534, loss: 0.043207628465035765\n",
      "timestep: 1535, loss: 0.03637668499979365\n",
      "timestep: 1536, loss: 0.0288510002242517\n",
      "timestep: 1537, loss: 0.024579623329311594\n",
      "timestep: 1538, loss: 0.024280031347344397\n",
      "timestep: 1539, loss: 0.026784944847699322\n",
      "timestep: 1540, loss: 0.029872234900719833\n",
      "timestep: 1541, loss: 0.03199300562067365\n",
      "timestep: 1542, loss: 0.03272235359136073\n",
      "timestep: 1543, loss: 0.03262048402154214\n",
      "timestep: 1544, loss: 0.0327279124112112\n",
      "timestep: 1545, loss: 0.03285723320208998\n",
      "timestep: 1546, loss: 0.03297904915233029\n",
      "timestep: 1547, loss: 0.03319434641001831\n",
      "timestep: 1548, loss: 0.0318612125615009\n",
      "timestep: 1549, loss: 0.030525343022703242\n",
      "timestep: 1550, loss: 0.030336011341647703\n",
      "timestep: 1551, loss: 0.0306716725809652\n",
      "timestep: 1552, loss: 0.03108740575960974\n",
      "timestep: 1553, loss: 0.03170453431054546\n",
      "timestep: 1554, loss: 0.03258958617241658\n",
      "timestep: 1555, loss: 0.033974005630543654\n",
      "timestep: 1556, loss: 0.03573022908362954\n",
      "timestep: 1557, loss: 0.03834702620050583\n",
      "timestep: 1558, loss: 0.04257667919225863\n",
      "timestep: 1559, loss: 0.05004143830813237\n",
      "timestep: 1560, loss: 0.05394603830831901\n",
      "timestep: 1561, loss: 0.05723139705922915\n",
      "timestep: 1562, loss: 0.05961501985802548\n",
      "timestep: 1563, loss: 0.0633869467053865\n",
      "timestep: 1564, loss: 0.06981270447804944\n",
      "timestep: 1565, loss: 0.06882457456485276\n",
      "timestep: 1566, loss: 0.07177090197066065\n",
      "timestep: 1567, loss: 0.07683368569748003\n",
      "timestep: 1568, loss: 0.0752277536697581\n",
      "timestep: 1569, loss: 0.08197930123443378\n",
      "timestep: 1570, loss: 0.0698933946203406\n",
      "timestep: 1571, loss: 0.09686205592763146\n",
      "timestep: 1572, loss: 0.07128352831082058\n",
      "timestep: 1573, loss: 0.09695691221067065\n",
      "timestep: 1574, loss: 0.07337418047085625\n",
      "timestep: 1575, loss: 0.11896514095218307\n",
      "timestep: 1576, loss: 0.07695359926262504\n",
      "timestep: 1577, loss: 0.15994628050464674\n",
      "timestep: 1578, loss: 0.10636005410067093\n",
      "timestep: 1579, loss: 0.2986664963307915\n",
      "timestep: 1580, loss: 0.21912350919436605\n",
      "timestep: 1581, loss: 0.6381230341448212\n",
      "timestep: 1582, loss: 0.35412311625313936\n",
      "timestep: 1583, loss: 0.5334388403035443\n",
      "timestep: 1584, loss: 0.07842480860085684\n",
      "timestep: 1585, loss: 0.21667639301524222\n",
      "timestep: 1586, loss: 0.2374529899067959\n",
      "timestep: 1587, loss: 0.2165585012184592\n",
      "timestep: 1588, loss: 0.2814364870464106\n",
      "timestep: 1589, loss: 0.31859030632426294\n",
      "timestep: 1590, loss: 0.37596817548045186\n",
      "timestep: 1591, loss: 0.4581251365212389\n",
      "timestep: 1592, loss: 0.5639186421332055\n",
      "timestep: 1593, loss: 0.7191004273732721\n",
      "timestep: 1594, loss: 0.927273064187048\n",
      "timestep: 1595, loss: 1.2908262359691138\n",
      "timestep: 1596, loss: 2.02826933111742\n",
      "timestep: 1597, loss: 3.2148557533289948\n",
      "timestep: 1598, loss: 2.616679790160178\n",
      "timestep: 1599, loss: 1.8944194692383063\n",
      "timestep: 1600, loss: 1.0467908281556122\n",
      "timestep: 1601, loss: 0.7447524838613923\n",
      "timestep: 1602, loss: 0.4473202000448932\n",
      "timestep: 1603, loss: 0.14945560746299982\n",
      "timestep: 1604, loss: 0.05895756646321956\n",
      "timestep: 1605, loss: 0.07597684616216412\n",
      "timestep: 1606, loss: 0.042682382228236664\n",
      "timestep: 1607, loss: 0.0269524831292285\n",
      "timestep: 1608, loss: 0.032410605045299044\n",
      "timestep: 1609, loss: 0.03631049156610185\n",
      "timestep: 1610, loss: 0.03271050028397838\n",
      "timestep: 1611, loss: 0.02947637208661259\n",
      "timestep: 1612, loss: 0.03020189336594048\n",
      "timestep: 1613, loss: 0.03187463768293263\n",
      "timestep: 1614, loss: 0.03114964205236791\n",
      "timestep: 1615, loss: 0.029233957227842798\n",
      "timestep: 1616, loss: 0.02862346702606691\n",
      "timestep: 1617, loss: 0.029983236166693152\n",
      "timestep: 1618, loss: 0.03147286171747912\n",
      "timestep: 1619, loss: 0.03115964034714027\n",
      "timestep: 1620, loss: 0.029590327223110502\n",
      "timestep: 1621, loss: 0.028655423060098016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 1622, loss: 0.029294042382312556\n",
      "timestep: 1623, loss: 0.030573009848203655\n",
      "timestep: 1624, loss: 0.03073461748135228\n",
      "timestep: 1625, loss: 0.02937055698094621\n",
      "timestep: 1626, loss: 0.02782476153167228\n",
      "timestep: 1627, loss: 0.02770931570149009\n",
      "timestep: 1628, loss: 0.029236034623001936\n",
      "timestep: 1629, loss: 0.0308303680985509\n",
      "timestep: 1630, loss: 0.030843787963262672\n",
      "timestep: 1631, loss: 0.029508560564838512\n",
      "timestep: 1632, loss: 0.02817469459028803\n",
      "timestep: 1633, loss: 0.028011780699901324\n",
      "timestep: 1634, loss: 0.02896844915650161\n",
      "timestep: 1635, loss: 0.030219679072036553\n",
      "timestep: 1636, loss: 0.031057369008957378\n",
      "timestep: 1637, loss: 0.031627147170409944\n",
      "timestep: 1638, loss: 0.03193785682737398\n",
      "timestep: 1639, loss: 0.032046895123988194\n",
      "timestep: 1640, loss: 0.032307185368239726\n",
      "timestep: 1641, loss: 0.03271090263160391\n",
      "timestep: 1642, loss: 0.03298935167342984\n",
      "timestep: 1643, loss: 0.03277827533116013\n",
      "timestep: 1644, loss: 0.03242445642421243\n",
      "timestep: 1645, loss: 0.03185746480878322\n",
      "timestep: 1646, loss: 0.031169454572711187\n",
      "timestep: 1647, loss: 0.030425493194225297\n",
      "timestep: 1648, loss: 0.029572382987332682\n",
      "timestep: 1649, loss: 0.028823482881465978\n",
      "timestep: 1650, loss: 0.027941095831127364\n",
      "timestep: 1651, loss: 0.026946062885261125\n",
      "timestep: 1652, loss: 0.026045728676589785\n",
      "timestep: 1653, loss: 0.025454718641115977\n",
      "timestep: 1654, loss: 0.02469588835064064\n",
      "timestep: 1655, loss: 0.02358360067192469\n",
      "timestep: 1656, loss: 0.02246614061646721\n",
      "timestep: 1657, loss: 0.02206091702838203\n",
      "timestep: 1658, loss: 0.02079762089009197\n",
      "timestep: 1659, loss: 0.01941971373787486\n",
      "timestep: 1660, loss: 0.018900942106437273\n",
      "timestep: 1661, loss: 0.018601188149531562\n",
      "timestep: 1662, loss: 0.01817506107561484\n",
      "timestep: 1663, loss: 0.017687462518752865\n",
      "timestep: 1664, loss: 0.01712053044758302\n",
      "timestep: 1665, loss: 0.016572178472520235\n",
      "timestep: 1666, loss: 0.016081633520002445\n",
      "timestep: 1667, loss: 0.01567321744795445\n",
      "timestep: 1668, loss: 0.015424239584936399\n",
      "timestep: 1669, loss: 0.015056939358451217\n",
      "timestep: 1670, loss: 0.014843447193060353\n",
      "timestep: 1671, loss: 0.014037868296913447\n",
      "timestep: 1672, loss: 0.015250631907390217\n",
      "timestep: 1673, loss: 0.013358442477073079\n",
      "timestep: 1674, loss: 0.016084371550207144\n",
      "timestep: 1675, loss: 0.013483026159409666\n",
      "timestep: 1676, loss: 0.028595698194368347\n",
      "timestep: 1677, loss: 0.03891685015192628\n",
      "timestep: 1678, loss: 0.17224115746471946\n",
      "timestep: 1679, loss: 0.47693907276086267\n",
      "timestep: 1680, loss: 1.7570118333857458\n",
      "timestep: 1681, loss: 1.5441012605041495\n",
      "timestep: 1682, loss: 0.10392224958885309\n",
      "timestep: 1683, loss: 0.13306094535758953\n",
      "timestep: 1684, loss: 0.049239463182712356\n",
      "timestep: 1685, loss: 0.025817335929812977\n",
      "timestep: 1686, loss: 0.04868315455406494\n",
      "timestep: 1687, loss: 0.059777853780519\n",
      "timestep: 1688, loss: 0.0496713835466926\n",
      "timestep: 1689, loss: 0.04736147038661856\n",
      "timestep: 1690, loss: 0.050024513325210844\n",
      "timestep: 1691, loss: 0.04884380467327005\n",
      "timestep: 1692, loss: 0.048556941181985794\n",
      "timestep: 1693, loss: 0.04761357814540252\n",
      "timestep: 1694, loss: 0.04641151262080178\n",
      "timestep: 1695, loss: 0.046330104562916814\n",
      "timestep: 1696, loss: 0.04604861237347255\n",
      "timestep: 1697, loss: 0.046264852070458475\n",
      "timestep: 1698, loss: 0.04636938291861865\n",
      "timestep: 1699, loss: 0.0462817750491586\n",
      "timestep: 1700, loss: 0.04772190236578008\n",
      "timestep: 1701, loss: 0.04989271376308869\n",
      "timestep: 1702, loss: 0.05020102591799524\n",
      "timestep: 1703, loss: 0.0505625906776379\n",
      "timestep: 1704, loss: 0.050841508973189085\n",
      "timestep: 1705, loss: 0.05201601542443133\n",
      "timestep: 1706, loss: 0.0498959975759632\n",
      "timestep: 1707, loss: 0.05735632054210076\n",
      "timestep: 1708, loss: 0.05080686615760299\n",
      "timestep: 1709, loss: 0.08342245423070187\n",
      "timestep: 1710, loss: 0.11697402426554868\n",
      "timestep: 1711, loss: 0.38880566528931576\n",
      "timestep: 1712, loss: 1.054138364818751\n",
      "timestep: 1713, loss: 2.1185500867321356\n",
      "timestep: 1714, loss: 0.34111037584862136\n",
      "timestep: 1715, loss: 0.31939869077754107\n",
      "timestep: 1716, loss: 0.2622018020069833\n",
      "timestep: 1717, loss: 0.3117714732669455\n",
      "timestep: 1718, loss: 0.4656868816207465\n",
      "timestep: 1719, loss: 0.5511286787557553\n",
      "timestep: 1720, loss: 0.6397303233614834\n",
      "timestep: 1721, loss: 0.8444309896414925\n",
      "timestep: 1722, loss: 1.2150026416563224\n",
      "timestep: 1723, loss: 1.7135091321401468\n",
      "timestep: 1724, loss: 1.8756056615882897\n",
      "timestep: 1725, loss: 1.0722236504242664\n",
      "timestep: 1726, loss: 1.1614216363973708\n",
      "timestep: 1727, loss: 0.6985786082878914\n",
      "timestep: 1728, loss: 0.8389835043705467\n",
      "timestep: 1729, loss: 1.1277247347647572\n",
      "timestep: 1730, loss: 0.7204526766183247\n",
      "timestep: 1731, loss: 0.47394465957990595\n",
      "timestep: 1732, loss: 0.31757813652922934\n",
      "timestep: 1733, loss: 0.3119733213060124\n",
      "timestep: 1734, loss: 0.1403899913755052\n",
      "timestep: 1735, loss: 0.05303269675366117\n",
      "timestep: 1736, loss: 0.07992237829370916\n",
      "timestep: 1737, loss: 0.09114212230588546\n",
      "timestep: 1738, loss: 0.04131679273185824\n",
      "timestep: 1739, loss: 0.01321743238430627\n",
      "timestep: 1740, loss: 0.031807540604456355\n",
      "timestep: 1741, loss: 0.05853633011575205\n",
      "timestep: 1742, loss: 0.03774740221744105\n",
      "timestep: 1743, loss: 0.017045954357883128\n",
      "timestep: 1744, loss: 0.009673468149716357\n",
      "timestep: 1745, loss: 0.040814726056396376\n",
      "timestep: 1746, loss: 0.05008841255705632\n",
      "timestep: 1747, loss: 0.034307090776585884\n",
      "timestep: 1748, loss: 0.016623386273864185\n",
      "timestep: 1749, loss: 0.007686196819360344\n",
      "timestep: 1750, loss: 0.04219444514334015\n",
      "timestep: 1751, loss: 0.06018783314748417\n",
      "timestep: 1752, loss: 0.045706616186738484\n",
      "timestep: 1753, loss: 0.03336305631102155\n",
      "timestep: 1754, loss: 0.009301823165929242\n",
      "timestep: 1755, loss: 0.01713425079298636\n",
      "timestep: 1756, loss: 0.05873093170652573\n",
      "timestep: 1757, loss: 0.06309160662521339\n",
      "timestep: 1758, loss: 0.04652491054680102\n",
      "timestep: 1759, loss: 0.03661977626427022\n",
      "timestep: 1760, loss: 0.016015842750197476\n",
      "timestep: 1761, loss: 0.005971063285753587\n",
      "timestep: 1762, loss: 0.024485625928660593\n",
      "timestep: 1763, loss: 0.04068314110627095\n",
      "timestep: 1764, loss: 0.03795345111247712\n",
      "timestep: 1765, loss: 0.03000275982742642\n",
      "timestep: 1766, loss: 0.024157737723411378\n",
      "timestep: 1767, loss: 0.01972872362585826\n",
      "timestep: 1768, loss: 0.01767971477384483\n",
      "timestep: 1769, loss: 0.01796587610085456\n",
      "timestep: 1770, loss: 0.018811037496938184\n",
      "timestep: 1771, loss: 0.019077772661358787\n",
      "timestep: 1772, loss: 0.019149642896428246\n",
      "timestep: 1773, loss: 0.019052291213310202\n",
      "timestep: 1774, loss: 0.01858008012873188\n",
      "timestep: 1775, loss: 0.01795189121534798\n",
      "timestep: 1776, loss: 0.017191375343701056\n",
      "timestep: 1777, loss: 0.01629754577324792\n",
      "timestep: 1778, loss: 0.015286884575347192\n",
      "timestep: 1779, loss: 0.014308118959831384\n",
      "timestep: 1780, loss: 0.013258144514101938\n",
      "timestep: 1781, loss: 0.012167214750096328\n",
      "timestep: 1782, loss: 0.011229939689551502\n",
      "timestep: 1783, loss: 0.010549706185425025\n",
      "timestep: 1784, loss: 0.009973219260966617\n",
      "timestep: 1785, loss: 0.009436383524827128\n",
      "timestep: 1786, loss: 0.00900339995705904\n",
      "timestep: 1787, loss: 0.008666704691499202\n",
      "timestep: 1788, loss: 0.008313969655999965\n",
      "timestep: 1789, loss: 0.008190383129508573\n",
      "timestep: 1790, loss: 0.007908117631189432\n",
      "timestep: 1791, loss: 0.007680658276951855\n",
      "timestep: 1792, loss: 0.007454247138140035\n",
      "timestep: 1793, loss: 0.007226656733737083\n",
      "timestep: 1794, loss: 0.00698024408518994\n",
      "timestep: 1795, loss: 0.006801291233253287\n",
      "timestep: 1796, loss: 0.0065731145388449866\n",
      "timestep: 1797, loss: 0.006514179847857174\n",
      "timestep: 1798, loss: 0.006118773971503854\n",
      "timestep: 1799, loss: 0.006928605453714685\n",
      "timestep: 1800, loss: 0.005479236588392589\n",
      "timestep: 1801, loss: 0.008455753251555105\n",
      "timestep: 1802, loss: 0.004848031616185473\n",
      "timestep: 1803, loss: 0.02017050462333679\n",
      "timestep: 1804, loss: 0.024416876027577753\n",
      "timestep: 1805, loss: 0.1596555708913314\n",
      "timestep: 1806, loss: 0.41648934214068556\n",
      "timestep: 1807, loss: 1.8650967277908377\n",
      "timestep: 1808, loss: 1.7541774748492032\n",
      "timestep: 1809, loss: 0.2264370102333835\n",
      "timestep: 1810, loss: 0.13054131868526272\n",
      "timestep: 1811, loss: 0.021463621035464415\n",
      "timestep: 1812, loss: 0.014446086800028268\n",
      "timestep: 1813, loss: 0.030577070810440164\n",
      "timestep: 1814, loss: 0.02437466587810576\n",
      "timestep: 1815, loss: 0.016677772905069235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 1816, loss: 0.018463373982809214\n",
      "timestep: 1817, loss: 0.01878354018580094\n",
      "timestep: 1818, loss: 0.017204370608443067\n",
      "timestep: 1819, loss: 0.01744986580291404\n",
      "timestep: 1820, loss: 0.016994208487676885\n",
      "timestep: 1821, loss: 0.016785075244538282\n",
      "timestep: 1822, loss: 0.016344519925506017\n",
      "timestep: 1823, loss: 0.01588035068182366\n",
      "timestep: 1824, loss: 0.01545714653204673\n",
      "timestep: 1825, loss: 0.015076790741028016\n",
      "timestep: 1826, loss: 0.014348171071763026\n",
      "timestep: 1827, loss: 0.01415240294813297\n",
      "timestep: 1828, loss: 0.013696030739120674\n",
      "timestep: 1829, loss: 0.013698332086259436\n",
      "timestep: 1830, loss: 0.012862015845651819\n",
      "timestep: 1831, loss: 0.01357490072870683\n",
      "timestep: 1832, loss: 0.011798328394667275\n",
      "timestep: 1833, loss: 0.015152488462725607\n",
      "timestep: 1834, loss: 0.011071020211049935\n",
      "timestep: 1835, loss: 0.03226634434646768\n",
      "timestep: 1836, loss: 0.046862439882478964\n",
      "timestep: 1837, loss: 0.26102452937025317\n",
      "timestep: 1838, loss: 0.7874955479415007\n",
      "timestep: 1839, loss: 2.774759887246677\n",
      "timestep: 1840, loss: 0.6716726210239051\n",
      "timestep: 1841, loss: 0.08238469597048631\n",
      "timestep: 1842, loss: 0.15775283087428635\n",
      "timestep: 1843, loss: 0.14017010383699688\n",
      "timestep: 1844, loss: 0.06525386916164604\n",
      "timestep: 1845, loss: 0.0763252863224786\n",
      "timestep: 1846, loss: 0.11056996687504111\n",
      "timestep: 1847, loss: 0.10856009991151555\n",
      "timestep: 1848, loss: 0.10552644677876205\n",
      "timestep: 1849, loss: 0.11255934870398412\n",
      "timestep: 1850, loss: 0.11674460990903848\n",
      "timestep: 1851, loss: 0.12044812497702165\n",
      "timestep: 1852, loss: 0.12658981525918322\n",
      "timestep: 1853, loss: 0.13386983069992722\n",
      "timestep: 1854, loss: 0.14194589997975005\n",
      "timestep: 1855, loss: 0.15276955419202368\n",
      "timestep: 1856, loss: 0.16463716195858516\n",
      "timestep: 1857, loss: 0.1739899963512342\n",
      "timestep: 1858, loss: 0.18652256284574395\n",
      "timestep: 1859, loss: 0.199296483046934\n",
      "timestep: 1860, loss: 0.21044326076172598\n",
      "timestep: 1861, loss: 0.2262969168352426\n",
      "timestep: 1862, loss: 0.24622456834838166\n",
      "timestep: 1863, loss: 0.268408107251415\n",
      "timestep: 1864, loss: 0.3000028418798069\n",
      "timestep: 1865, loss: 0.34243796616299327\n",
      "timestep: 1866, loss: 0.39455902634502227\n",
      "timestep: 1867, loss: 0.46949543887810014\n",
      "timestep: 1868, loss: 0.5836250229313579\n",
      "timestep: 1869, loss: 0.7666175584867344\n",
      "timestep: 1870, loss: 1.069149396945756\n",
      "timestep: 1871, loss: 1.5149913301336495\n",
      "timestep: 1872, loss: 1.789204165990835\n",
      "timestep: 1873, loss: 1.178436718379678\n",
      "timestep: 1874, loss: 1.1857838113120525\n",
      "timestep: 1875, loss: 0.7236962520079496\n",
      "timestep: 1876, loss: 0.7405590591141585\n",
      "timestep: 1877, loss: 1.2975737441537745\n",
      "timestep: 1878, loss: 0.9270722294515068\n",
      "timestep: 1879, loss: 0.5919320140337394\n",
      "timestep: 1880, loss: 0.3754299582434918\n",
      "timestep: 1881, loss: 0.39579896626839095\n",
      "timestep: 1882, loss: 0.19095764838221327\n",
      "timestep: 1883, loss: 0.04676845377971318\n",
      "timestep: 1884, loss: 0.09123967863605524\n",
      "timestep: 1885, loss: 0.0946564174844483\n",
      "timestep: 1886, loss: 0.04531773354297081\n",
      "timestep: 1887, loss: 0.014260992793937739\n",
      "timestep: 1888, loss: 0.04291777922950892\n",
      "timestep: 1889, loss: 0.05813035689513324\n",
      "timestep: 1890, loss: 0.03853234763804626\n",
      "timestep: 1891, loss: 0.015215588886833215\n",
      "timestep: 1892, loss: 0.02184897677089422\n",
      "timestep: 1893, loss: 0.0482109519702113\n",
      "timestep: 1894, loss: 0.04452825951479588\n",
      "timestep: 1895, loss: 0.02839463731160995\n",
      "timestep: 1896, loss: 0.009273393751586492\n",
      "timestep: 1897, loss: 0.02538481795185701\n",
      "timestep: 1898, loss: 0.05088577021329188\n",
      "timestep: 1899, loss: 0.04808275326322577\n",
      "timestep: 1900, loss: 0.038740552271453295\n",
      "timestep: 1901, loss: 0.013674951128053796\n",
      "timestep: 1902, loss: 0.012332187642731519\n",
      "timestep: 1903, loss: 0.04642189757359843\n",
      "timestep: 1904, loss: 0.05555155503951656\n",
      "timestep: 1905, loss: 0.05069068600584272\n",
      "timestep: 1906, loss: 0.04338833135708095\n",
      "timestep: 1907, loss: 0.016159590555391772\n",
      "timestep: 1908, loss: 0.007023763380836199\n",
      "timestep: 1909, loss: 0.024740477413638984\n",
      "timestep: 1910, loss: 0.04932669002965575\n",
      "timestep: 1911, loss: 0.05022031744321218\n",
      "timestep: 1912, loss: 0.04731997006254406\n",
      "timestep: 1913, loss: 0.04401412172084057\n",
      "timestep: 1914, loss: 0.03163357389628299\n",
      "timestep: 1915, loss: 0.019133542850706828\n",
      "timestep: 1916, loss: 0.014719768963870616\n",
      "timestep: 1917, loss: 0.015541911819458031\n",
      "timestep: 1918, loss: 0.017202218595075564\n",
      "timestep: 1919, loss: 0.018675765176524504\n",
      "timestep: 1920, loss: 0.01967795351867124\n",
      "timestep: 1921, loss: 0.020500804895544875\n",
      "timestep: 1922, loss: 0.020805786718016576\n",
      "timestep: 1923, loss: 0.020944333722086785\n",
      "timestep: 1924, loss: 0.020609439752357416\n",
      "timestep: 1925, loss: 0.0202108113677224\n",
      "timestep: 1926, loss: 0.019411180265033395\n",
      "timestep: 1927, loss: 0.018704055433079965\n",
      "timestep: 1928, loss: 0.017904782105706934\n",
      "timestep: 1929, loss: 0.016986867089459175\n",
      "timestep: 1930, loss: 0.01637880134529398\n",
      "timestep: 1931, loss: 0.015670465808131164\n",
      "timestep: 1932, loss: 0.01483123705915599\n",
      "timestep: 1933, loss: 0.01405160460940656\n",
      "timestep: 1934, loss: 0.013340322704676954\n",
      "timestep: 1935, loss: 0.012450638949811877\n",
      "timestep: 1936, loss: 0.011757969885719179\n",
      "timestep: 1937, loss: 0.010982358302287649\n",
      "timestep: 1938, loss: 0.010339894974718706\n",
      "timestep: 1939, loss: 0.009467979964319638\n",
      "timestep: 1940, loss: 0.008897078925170922\n",
      "timestep: 1941, loss: 0.00851363531455843\n",
      "timestep: 1942, loss: 0.008443312417609082\n",
      "timestep: 1943, loss: 0.007988307761276705\n",
      "timestep: 1944, loss: 0.007905772411901275\n",
      "timestep: 1945, loss: 0.007228724671055744\n",
      "timestep: 1946, loss: 0.007058817718968785\n",
      "timestep: 1947, loss: 0.006219474640587902\n",
      "timestep: 1948, loss: 0.006769368241188636\n",
      "timestep: 1949, loss: 0.005246462009234265\n",
      "timestep: 1950, loss: 0.007567927901388342\n",
      "timestep: 1951, loss: 0.00412775178309089\n",
      "timestep: 1952, loss: 0.014666010386189983\n",
      "timestep: 1953, loss: 0.013814444948368108\n",
      "timestep: 1954, loss: 0.09342701661611402\n",
      "timestep: 1955, loss: 0.2141436573545933\n",
      "timestep: 1956, loss: 1.2093833936268883\n",
      "timestep: 1957, loss: 2.2497416872713902\n",
      "timestep: 1958, loss: 0.9844467648735219\n",
      "timestep: 1959, loss: 0.2935618421424879\n",
      "timestep: 1960, loss: 0.21196155600732505\n",
      "timestep: 1961, loss: 0.1588835929330453\n",
      "timestep: 1962, loss: 0.09496273016353567\n",
      "timestep: 1963, loss: 0.0664895747309043\n",
      "timestep: 1964, loss: 0.04421104786067082\n",
      "timestep: 1965, loss: 0.037480129238486586\n",
      "timestep: 1966, loss: 0.03366493005199837\n",
      "timestep: 1967, loss: 0.027147661607986682\n",
      "timestep: 1968, loss: 0.02390076097702084\n",
      "timestep: 1969, loss: 0.022594247795963043\n",
      "timestep: 1970, loss: 0.020375204016995163\n",
      "timestep: 1971, loss: 0.018012274743504513\n",
      "timestep: 1972, loss: 0.016989390624596718\n",
      "timestep: 1973, loss: 0.015468997041252984\n",
      "timestep: 1974, loss: 0.013887839411045372\n",
      "timestep: 1975, loss: 0.013667490677613704\n",
      "timestep: 1976, loss: 0.012130705897326637\n",
      "timestep: 1977, loss: 0.011915033889650093\n",
      "timestep: 1978, loss: 0.010972237730258107\n",
      "timestep: 1979, loss: 0.010179184566479049\n",
      "timestep: 1980, loss: 0.00989125619861928\n",
      "timestep: 1981, loss: 0.008652467041113858\n",
      "timestep: 1982, loss: 0.009080272317573428\n",
      "timestep: 1983, loss: 0.0073219477605009934\n",
      "timestep: 1984, loss: 0.009029568027279503\n",
      "timestep: 1985, loss: 0.005778482118549103\n",
      "timestep: 1986, loss: 0.010768783649611948\n",
      "timestep: 1987, loss: 0.00469365434481559\n",
      "timestep: 1988, loss: 0.024847137318739696\n",
      "timestep: 1989, loss: 0.028245461321949453\n",
      "timestep: 1990, loss: 0.18328598160907578\n",
      "timestep: 1991, loss: 0.48531499933028693\n",
      "timestep: 1992, loss: 1.8499826536850195\n",
      "timestep: 1993, loss: 0.9585835100618332\n",
      "timestep: 1994, loss: 0.1624872808024673\n",
      "timestep: 1995, loss: 0.13119447851786162\n",
      "timestep: 1996, loss: 0.01667008009240211\n",
      "timestep: 1997, loss: 0.02040651749092588\n",
      "timestep: 1998, loss: 0.03113575982142026\n",
      "timestep: 1999, loss: 0.013914478997989756\n",
      "[544.3663398203461, 323.19237847023277, 188.44672205512833, 61.52727899393938, 11.195009901822754, 6.824624032648865, 5.405495559792976, 4.822565872250168, 4.526994884341434, 4.285968147080569, 4.055167478450598, 3.8029284281787197, 3.8041948848119578, 3.725584014654288, 3.625251269081432, 3.6558406503539107, 3.6119282440120304, 3.5729319346997896, 3.556982386205365, 3.520808685512046, 3.4747455463912518, 3.4396866640301074, 3.3796175946916964, 3.3285153461029977, 3.2334941254001035, 3.179861926224426, 3.0926932134779834, 3.058591679108858, 2.9972084354765967, 2.968773987198952, 2.9224385323044486, 2.8190603516800063, 2.737410248691464, 2.717401117902955, 2.725517508313966, 2.7158642877786647, 2.671229422860358, 2.6288853427721253, 2.556790168248595, 2.480007764774136, 2.396572273108721, 2.476074403859246, 2.365374553808783, 2.3014326626162473, 2.2463159323247344, 2.3084411104256097, 2.2908837080706346, 2.2824482383181337, 2.1796790944362225, 1.8438446137880986, 2.1413841933556665, 2.0840607778687774, 2.09532021710073, 2.123414180056135, 2.0765847689149743, 2.048414630274327, 2.0439566634762465, 2.0063555854859856, 1.9177114099788994, 1.779397611664014, 1.8708125184199458, 1.8602836270988015, 1.9060195383154712, 1.8367326075531694, 1.3909548271319057, 1.872860957095867, 1.7434749301366366, 1.7459246892707734, 1.506561281425334, 1.8314410759240192, 1.572297471553751, 1.6963515928278377, 1.5276914468145057, 0.6060656585959046, 1.4253517755829086, 1.7607645049571985, 1.019395728929779, 2.0898946047224456, 0.9462160401661778, 1.3757672431622738, 1.9205199554999124, 0.6178212502131143, 2.3892064812119815, 0.6433562214284954, 0.5203029732865256, 3.6535239975143585, 0.13092524724264906, 0.09788106023997324, 0.06002836651612284, 0.05659815682123484, 0.0531510014884836, 0.04975141437990326, 0.04650082222895553, 0.045461531714487295, 0.04594118363276112, 0.04736044035718592, 0.07631488739738503, 0.08447345733161273, 0.10170303083327258, 0.14511908180974975, 0.17597607847455182, 0.1719272724579173, 0.15735785373742855, 0.0988420698543297, 0.06358677722927246, 0.04752193622154841, 1.8661664133553935, 0.19638451530170364, 0.07726184523678666, 0.08527143495689865, 0.06033706878211886, 0.03053391234260815, 0.033171073688158824, 0.02282479596219572, 0.04190220947657892, 0.04044693441786831, 0.03743022917773355, 0.08515024068156823, 0.03844381126476471, 0.025990987808615987, 0.3178153469989492, 0.026894016665619445, 0.14675782958380548, 0.3388963001030165, 0.04495992633355244, 0.08166200060238361, 0.09814529129311188, 0.0945513029669618, 1.055192059203405, 0.10836391823004009, 1.3565831956590335, 0.2927724703427042, 0.4579044662766521, 0.8650536585661347, 0.10588371885798765, 0.10683970609227184, 0.03883576072531725, 0.10051927644703397, 0.15686328324913668, 0.2855202473713565, 0.23652854601733075, 0.1977582097446686, 0.24838712977608726, 0.3284072370831929, 0.17930689628134067, 0.09095904885385081, 0.09703996560773649, 0.140471894080462, 0.23545190335539362, 0.3551738221347408, 2.741519702380145, 0.029105867876647182, 0.024271016439865897, 0.016989523989783328, 0.029872234900719833, 0.030336011341647703, 0.05394603830831901, 0.0698933946203406, 0.21912350919436605, 0.37596817548045186, 1.0467908281556122, 0.03271050028397838, 0.029590327223110502, 0.030843787963262672, 0.032307185368239726, 0.027941095831127364, 0.018900942106437273, 0.014843447193060353, 1.7570118333857458, 0.050024513325210844, 0.04772190236578008, 0.11697402426554868, 0.6397303233614834, 0.7204526766183247, 0.031807540604456355, 0.04219444514334015, 0.016015842750197476, 0.018811037496938184, 0.013258144514101938, 0.007908117631189432, 0.005479236588392589, 0.13054131868526272, 0.016994208487676885, 0.012862015845651819, 0.6716726210239051, 0.11674460990903848, 0.21044326076172598, 1.069149396945756, 0.3754299582434918, 0.03853234763804626, 0.038740552271453295, 0.04932669002965575, 0.01967795351867124, 0.01637880134529398, 0.008897078925170922, 0.007567927901388342, 0.21196155600732505, 0.020375204016995163, 0.00989125619861928, 0.18328598160907578]\n",
      "0.14564981694561438\n"
     ]
    }
   ],
   "source": [
    "# net = SecondNet(vertex_dim, edge_dim, hidden_dim).double()\n",
    "net = SimpleNet(vertex_dim, edge_dim, hidden_dim, p = 0.0).double()\n",
    "\n",
    "\n",
    "losses = train(net, train_loader, lr = 0.002, iterations = 2000, criterion = train_criterion, verbose = True)\n",
    "print(losses[::10])\n",
    "\n",
    "loss = test(net, test_loader, test_criterion)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-110.7541, -148.3292,  -87.5184, -148.3689, -111.8256, -131.5436],\n",
      "       dtype=torch.float64, grad_fn=<SqueezeBackward1>)\n",
      "tensor([-110.6895, -148.0932,  -87.4793, -148.3788, -111.5629, -131.4503],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for data in test_loader:\n",
    "    output = net(data)\n",
    "    loss = test_criterion(output, data.y.double())\n",
    "    print(output)\n",
    "    print(data.y.double())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:prime] *",
   "language": "python",
   "name": "conda-env-prime-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
